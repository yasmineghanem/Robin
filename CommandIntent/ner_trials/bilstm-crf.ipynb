{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pickle, glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Random librairies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# Keras and tensorflow librairies\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, TimeDistributed, Reshape, Bidirectional, concatenate, Flatten, Layer\n",
    "from keras_contrib.layers import CRF\n",
    "from keras.utils import plot_model, pad_sequences, to_categorical\n",
    "from keras import backend as K\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras_contrib.metrics import crf_accuracy\n",
    "\n",
    "# Sklearn librairies\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version:  3.4.1\n",
      "Tensorflow version:  2.16.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Keras version: \", keras.__version__)\n",
    "print(\"Tensorflow version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_slice(x, start, size):\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return x[:, start[1]:start[1] + size[1]]\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported backend\")\n",
    "\n",
    "# Patch the custom slice method into Keras backend\n",
    "K.slice = custom_slice\n",
    "\n",
    "class CustomCRF(CRF):\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        if mask is not None and self.learn_mode == 'join':\n",
    "            return tf.reduce_any(mask, axis=1)  # Use tf.reduce_any instead of K.any\n",
    "        return mask\n",
    "\n",
    "# Custom layer to wrap tf.reduce_any\n",
    "class MaskAny(Layer):\n",
    "    def call(self, inputs):\n",
    "        return tf.reduce_any(inputs, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Read and Describe Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Sentence #', 'Word', 'Tag', 'Intent'], dtype='object')\n",
      "Columns: Index(['Sentence #', 'Word', 'Tag', 'Intent'], dtype='object') Index(['Sentence #', 'Word', 'Tag', 'Intent'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is  approved  equals  clustering  algorithms</td>\n",
       "      <td>[ B-VAR,  I-VAR,  O,  B-VAL,  I-VAL]</td>\n",
       "      <td>[ variable_declaration,  variable_declaration,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>file  path  equals  2023</td>\n",
       "      <td>[ B-VAR,  I-VAR,  O,  B-VAL]</td>\n",
       "      <td>[ variable_declaration,  variable_declaration,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>user  id  equals  REGRESSION  MODELS</td>\n",
       "      <td>[ B-VAR,  I-VAR,  O,  B-VAL,  I-VAL]</td>\n",
       "      <td>[ variable_declaration,  variable_declaration,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>temp  equals  car  bus  train  plane  bicycle</td>\n",
       "      <td>[ B-VAR,  O,  B-VAL,  B-VAL,  B-VAL,  B-VAL,  ...</td>\n",
       "      <td>[ variable_declaration,  variable_declaration,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>time  elapsed  equals  -555555</td>\n",
       "      <td>[ B-VAR,  I-VAR,  O,  B-VAL]</td>\n",
       "      <td>[ variable_declaration,  variable_declaration,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence #                                            Word   \n",
       "0           0    is  approved  equals  clustering  algorithms  \\\n",
       "1           1                        file  path  equals  2023   \n",
       "2           2            user  id  equals  REGRESSION  MODELS   \n",
       "3           3   temp  equals  car  bus  train  plane  bicycle   \n",
       "4           4                  time  elapsed  equals  -555555   \n",
       "\n",
       "                                                 Tag   \n",
       "0               [ B-VAR,  I-VAR,  O,  B-VAL,  I-VAL]  \\\n",
       "1                       [ B-VAR,  I-VAR,  O,  B-VAL]   \n",
       "2               [ B-VAR,  I-VAR,  O,  B-VAL,  I-VAL]   \n",
       "3  [ B-VAR,  O,  B-VAL,  B-VAL,  B-VAL,  B-VAL,  ...   \n",
       "4                       [ B-VAR,  I-VAR,  O,  B-VAL]   \n",
       "\n",
       "                                              Intent  \n",
       "0  [ variable_declaration,  variable_declaration,...  \n",
       "1  [ variable_declaration,  variable_declaration,...  \n",
       "2  [ variable_declaration,  variable_declaration,...  \n",
       "3  [ variable_declaration,  variable_declaration,...  \n",
       "4  [ variable_declaration,  variable_declaration,...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_dataset():\n",
    "    data = pd.read_csv('./ner_dataset/ner_dataset.csv', encoding='latin1')\n",
    "\n",
    "    # remove white spaces from column names\n",
    "    data.columns = data.columns.str.strip()\n",
    "\n",
    "    print(data.columns)\n",
    "    # print(data.columns)\n",
    "    # Group by 'Sentence #' and aggregate\n",
    "    grouped_data = data.groupby('Sentence #').agg({\n",
    "        'Word': lambda x: ' '.join(x),  # Join words into a single sentence\n",
    "        'Tag': lambda x: list(x),       # Collect tags into a list\n",
    "        'Intent': lambda x: x     # Collect intents into a list\n",
    "    }).reset_index()  # Reset index to make 'Sentence #' a regular column\n",
    "\n",
    "    return data, grouped_data\n",
    "\n",
    "\n",
    "def prepare_data(dataframe):\n",
    "    dataset = []\n",
    "    for index, row in dataframe.iterrows():\n",
    "        sentence = row['Word']\n",
    "        tags = row['Tag']\n",
    "        intents = row['Intent'][0]\n",
    "        dataset.append((sentence, tags, intents))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "data, grouped_data = read_dataset()\n",
    "\n",
    "prepared_dataset = prepare_data(grouped_data)\n",
    "\n",
    "print(\"Columns:\", data.columns, grouped_data.columns)\n",
    "\n",
    "data.head()\n",
    "grouped_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the dataset: 304\n",
      "Number of unique tags in the dataset: 6\n",
      "Number of unique intents in the dataset: 1\n",
      "Unique tags in the dataset: [' B-VAR' ' I-VAR' ' O' ' B-VAL' ' I-VAL' ' B-TYPE']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique words in the dataset:\", len(data['Word'].unique()) )# number of unique words in the dataset\n",
    "print(\"Number of unique tags in the dataset:\", len(data['Tag'].unique())) # number of unique tags in the dataset\n",
    "print(\"Number of unique intents in the dataset:\", len(data['Intent'].unique())) #number of unique intents in the dataset\n",
    "\n",
    "print(\"Unique tags in the dataset:\", data['Tag'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting unique words and labels from data\n",
    "words = list(data['Word'].unique())\n",
    "tags = list(data['Tag'].unique())\n",
    "# Dictionary word:index pair\n",
    "# word is key and its value is corresponding index\n",
    "word_to_index = {word.strip() : i + 2 for i, word in enumerate(words)}\n",
    "word_to_index[\"UNK\"] = 1\n",
    "word_to_index[\"PAD\"] = 0\n",
    "\n",
    "# Dictionary lable:index pair\n",
    "# label is key and value is index.\n",
    "tag_to_index = {tag.strip() : i + 1 for i, tag in enumerate(tags)}\n",
    "tag_to_index[\"PAD\"] = 0\n",
    "\n",
    "idx2word = {i: word for word, i in word_to_index.items()}\n",
    "idx2tag = {i: tag for tag, i in tag_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word deckare is identified by the index: 79\n",
      "The label B-VAR for the variable is identified by the index: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"The word deckare is identified by the index: {}\".format(word_to_index[\"declare\"]))\n",
    "print(\"The label B-VAR for the variable is identified by the index: {}\".format(tag_to_index[\"B-VAR\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('is', 'B-VAR', 'variable_declaration'), ('approved', 'I-VAR', 'variable_declaration'), ('equals', 'O', 'variable_declaration'), ('clustering', 'B-VAL', 'variable_declaration'), ('algorithms', 'I-VAL', 'variable_declaration')]\n",
      "[('is', 'B-VAR', 'variable_declaration'), ('approved', 'I-VAR', 'variable_declaration'), ('equals', 'O', 'variable_declaration'), ('clustering', 'B-VAL', 'variable_declaration'), ('algorithms', 'I-VAL', 'variable_declaration')]\n",
      "('is  approved  equals  clustering  algorithms', ['B-VAR', 'I-VAR', 'O', 'B-VAL', 'I-VAL'], 'variable_declaration')\n"
     ]
    }
   ],
   "source": [
    "# This is a class te get sentence. The each sentence will be list of tuples with its tag and pos.\n",
    "class Sentence(object):\n",
    "    def __init__(self, df):\n",
    "        self.n_sent = 0\n",
    "        self.df = df\n",
    "        self.empty = False\n",
    "        agg = lambda s : [(word.strip(), tag.strip(), intent.strip()) for word, tag, intent in zip(s['Word'].values.tolist(),\n",
    "                                                       s['Tag'].values.tolist(),\n",
    "                                                       s['Intent'].values.tolist())]\n",
    "        self.grouped = self.df.groupby(\"Sentence #\").apply(agg)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "        \n",
    "    def get_text(self):\n",
    "        try:\n",
    "            s = self.grouped[self.n_sent]\n",
    "            self.n_sent +=1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def records_to_tuples(self):\n",
    "        dataset = []\n",
    "\n",
    "        grouped_data = data.groupby('Sentence #').agg({\n",
    "        'Word': lambda x: ' '.join(x),  # Join words into a single sentence\n",
    "        'Tag': lambda x: list(x.str.strip()),       # Collect tags into a list\n",
    "        'Intent': lambda x: x.str.strip()     # Collect intents into a list\n",
    "        }).reset_index()\n",
    "\n",
    "        for _, row in grouped_data.iterrows():\n",
    "            sentence = row['Word'][1:]\n",
    "            tags = row['Tag']\n",
    "            intents = row['Intent'][0]\n",
    "            dataset.append((sentence, tags, intents))\n",
    "\n",
    "        return dataset\n",
    "        \n",
    "#Displaying one full sentence\n",
    "getter = Sentence(data)\n",
    "sentences = [''.join([s[0] for s in sentence]) for sentence in getter.sentences]\n",
    "sentences[0]\n",
    "\n",
    "#sentence with its pos and tag.\n",
    "sent = getter.get_text()\n",
    "print(sent)\n",
    "\n",
    "sentences = getter.sentences\n",
    "print(sentences[0])\n",
    "\n",
    "other_sentences = getter.records_to_tuples()\n",
    "\n",
    "print(other_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Preparation For Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 306\n",
      "Tag to index: {'B-VAR': 1, 'I-VAR': 2, 'O': 3, 'B-VAL': 4, 'I-VAL': 5, 'B-TYPE': 6, 'PAD': 0}\n",
      "Index to tag: {0: 'PAD', 1: 'B-VAR', 2: 'I-VAR', 3: 'O', 4: 'B-VAL', 5: 'I-VAL', 6: 'B-TYPE'}\n",
      "Training data size: 148\n",
      "Training data sample: [([2, 3, 4, 5, 6], [1, 2, 3, 4, 5], 'variable declaration'), ([7, 8, 4, 9], [1, 2, 3, 4], 'variable declaration'), ([10, 11, 4, 12, 13], [1, 2, 3, 4, 5], 'variable declaration'), ([14, 4, 15, 16, 17, 18, 19], [1, 3, 4, 4, 4, 4, 4], 'variable declaration'), ([20, 21, 4, 22], [1, 2, 3, 4], 'variable declaration')]\n",
      "Padded sentences shape: (148, 20)\n",
      "Padded tags shape: (148, 20)\n",
      "Categorial tags shape: (148, 20, 7)\n"
     ]
    }
   ],
   "source": [
    "# We should convert each sentence to integers\n",
    "#Getting unique words and labels from data -> our vocab\n",
    "words = list(data['Word'].unique())\n",
    "tags = list(data['Tag'].unique())\n",
    "\n",
    "# 1. Each word to integer\n",
    "# word is key and its value is corresponding index\n",
    "word_to_index = {word.strip() : i + 2 for i, word in enumerate(words)}\n",
    "word_to_index['UNK'] = 1\n",
    "word_to_index['PAD'] = 0\n",
    "\n",
    "vocab_size = len(word_to_index)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "# 2. Each label to integer\n",
    "# label is key and value is index.\n",
    "tag_to_index = {tag.strip() : i + 1 for i, tag in enumerate(tags)}\n",
    "tag_to_index['PAD'] = 0\n",
    "\n",
    "print(\"Tag to index:\", tag_to_index)\n",
    "\n",
    "# conver _ to index to index_ to word\n",
    "index_to_word = dict(sorted({i: word for word, i in word_to_index.items()}.items(), key=lambda item: item[0]))\n",
    "index_to_tag = dict(sorted({i: tag for tag, i in tag_to_index.items()}.items(), key=lambda item: item[0]))\n",
    "\n",
    "print(\"Index to tag:\", index_to_tag)\n",
    "\n",
    "# 3. Each example to a list of integers\n",
    "training_data = [([word_to_index.get(word, word_to_index['UNK']) for word in sentence.split()], \n",
    "                   [tag_to_index[tag] for tag in tags], \n",
    "                   intent.replace('_', ' ')) for sentence, tags, intent in other_sentences]\n",
    "\n",
    "print(\"Training data size:\", len(training_data))\n",
    "print(\"Training data sample:\", training_data[0:5])\n",
    "\n",
    "# 4. Each sentence should be padded to have same length\n",
    "padded_sentences = pad_sequences([sentence for sentence, _, _ in training_data], padding='post')\n",
    "padded_tags = pad_sequences([tags for _, tags, _ in training_data], padding='post')\n",
    "\n",
    "categorial_tags = [to_categorical(tags, num_classes=len(tag_to_index)) for tags in padded_tags]\n",
    "\n",
    "print(\"Padded sentences shape:\", padded_sentences.shape)\n",
    "print(\"Padded tags shape:\", padded_tags.shape)\n",
    "print(\"Categorial tags shape:\", np.array(categorial_tags).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training input data:  (133, 20)\n",
      "Size of training labels:  (133, 20, 7)\n",
      "Size of testing input data:  (15, 20)\n",
      "Size of testing labels:  (15, 20, 7)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(padded_sentences, categorial_tags, test_size = 0.1)\n",
    "\n",
    "print(\"Size of training input data: \", x_train.shape)\n",
    "print(\"Size of training labels: \", np.array(y_train).shape)\n",
    "print(\"Size of testing input data: \", x_test.shape)\n",
    "print(\"Size of testing labels: \", np.array(y_test).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length: 20\n"
     ]
    }
   ],
   "source": [
    "# Model Parameters\n",
    "epochs = 10\n",
    "max_sentence_length = max([len(sentence.split()) for sentence, _, _ in prepared_dataset])\n",
    "print(\"Max sentence length:\", max_sentence_length)\n",
    "number_of_tags = len(tag_to_index) \n",
    "vocab_size = len(word_to_index)\n",
    "word_embedding = 100\n",
    "intent_embedding = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_5       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_5         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">30,600</span> │ input_layer_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_5         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_5     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">60,400</span> │ embedding_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │ not_equal_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_5  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">707</span> │ bidirectional_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │ not_equal_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ custom_crf_2        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">119</span> │ time_distributed… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CustomCRF</span>)         │                   │            │ not_equal_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_5       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_5         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m100\u001b[0m)   │     \u001b[38;5;34m30,600\u001b[0m │ input_layer_5[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_5         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ input_layer_5[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_5     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m100\u001b[0m)   │     \u001b[38;5;34m60,400\u001b[0m │ embedding_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │ not_equal_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_5  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m7\u001b[0m)     │        \u001b[38;5;34m707\u001b[0m │ bidirectional_5[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │ not_equal_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ custom_crf_2        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m7\u001b[0m)     │        \u001b[38;5;34m119\u001b[0m │ time_distributed… │\n",
       "│ (\u001b[38;5;33mCustomCRF\u001b[0m)         │                   │            │ not_equal_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">91,826</span> (358.70 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m91,826\u001b[0m (358.70 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">91,826</span> (358.70 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m91,826\u001b[0m (358.70 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input = Input(shape=(max_sentence_length,)) # should be the embeding of the word along with the intent\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=word_embedding, mask_zero=True)(input) # word embedding layer\n",
    "bilst_layer = Bidirectional(LSTM(units=50, return_sequences=True, recurrent_dropout=0.2))(embedding_layer) # BI-LSTM layer\n",
    "dense_layer = TimeDistributed(Dense(units=number_of_tags ,activation='relu'))(bilst_layer)\n",
    "crf = CustomCRF(number_of_tags)\n",
    "output = crf(dense_layer)\n",
    "\n",
    "final_model = Model(inputs=input ,outputs=output)\n",
    "\n",
    "final_model.compile(optimizer='rmsprop', loss=crf_loss, metrics=[crf_accuracy, 'accuracy'])\n",
    "\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "Exception encountered when calling TimeDistributed.call().\n\n\u001b[1mUsing a symbolic `tf.Tensor` as a Python `bool` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\u001b[0m\n\nArguments received by TimeDistributed.call():\n  • inputs=tf.Tensor(shape=(None, 20, 100), dtype=float32)\n  • training=True\n  • mask=tf.Tensor(shape=(None, 20), dtype=bool)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfinal_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yazmi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\yazmi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[1;31mOperatorNotAllowedInGraphError\u001b[0m: Exception encountered when calling TimeDistributed.call().\n\n\u001b[1mUsing a symbolic `tf.Tensor` as a Python `bool` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\u001b[0m\n\nArguments received by TimeDistributed.call():\n  • inputs=tf.Tensor(shape=(None, 20, 100), dtype=float32)\n  • training=True\n  • mask=tf.Tensor(shape=(None, 20), dtype=bool)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "final_model.fit(x_train, np.array(y_train), epochs=epochs, validation_split=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
