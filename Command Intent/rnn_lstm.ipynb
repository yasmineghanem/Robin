{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial 1 : RNN (LSTM) \n",
    "can handle sequential data (sentences) effectively\n",
    "1. Embedding layer\n",
    "2. LSTM layer (sequence processing)\n",
    "3. Two dense layers for classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import utils\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique intents: 17\n",
      "Number of examples: 1459\n",
      "Number of examples: 1459\n",
      "Unique intents: ['Variable Declaration', 'Function Declaration', 'Class Declaration', 'Assignment Operation', 'Conditional Statement', 'Iterative Statement', 'Array Operation', 'Bitwise Operation', 'Mathematical Operation', 'Membership Operation', 'Casting', 'IO Operation', 'Assertion', 'Libraries', 'File System', 'IDE Operation', 'Comments']\n",
      "Samples: ['is approved equals clustering algorithms', 'file path equals', 'user id equals REGRESSION MODELS', 'temp equals car bus train plane bicycle', 'time elapsed equals']\n"
     ]
    }
   ],
   "source": [
    "dataset_path = './intent_detection_dataset/intents_pattern.json'\n",
    "unique_intents, corpus, corpus_intents = utils.load_data(dataset_path)\n",
    "\n",
    "# print shapes and sizes of the dataset\n",
    "print('Number of unique intents:', len(unique_intents))\n",
    "# print('Number of responses:', len(responses))\n",
    "print('Number of examples:', len(corpus))\n",
    "print('Number of examples:', len(corpus_intents))\n",
    "\n",
    "# print samples of the dataset\n",
    "print('Unique intents:', unique_intents)\n",
    "# print('Responses:', responses)\n",
    "print('Samples:', corpus[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Tokenizing and Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'<unk>': 1, 'the': 2, 'and': 3, 'a': 4, 'to': 5, 'is': 6, 'user': 7, 'in': 8, 'name': 9, 'value': 10, 'equal': 11, 'variable': 12, 'if': 13, 'list': 14, 'with': 15, 'for': 16, 'it': 17, 'check': 18, 'than': 19, 'new': 20, 'loop': 21, 'whether': 22, 'while': 23, 'array': 24, 'bitwise': 25, 'make': 26, 'not': 27, 'from': 28, 'as': 29, 'set': 30, 'parameters': 31, 'that': 32, 'type': 33, 'of': 34, 'create': 35, 'algorithm': 36, 'date': 37, 'equals': 38, 'time': 39, 'or': 40, 'product': 41, 'on': 42, 'define': 43, 'declare': 44, 'are': 45, 'initialize': 46, 'less': 47, 'write': 48, 'data': 49, 'under': 50, 'range': 51, 'assign': 52, 'step': 53, 'index': 54, 'return': 55, 'returns': 56, 'get': 57, 'class': 58, 'key': 59, 'greater': 60, 'id': 61, 'retrieve': 62, 'dictionary': 63, 'labelled': 64, 'iterate': 65, 'identified': 66, 'called': 67, 'config': 68, 'shift': 69, 'count': 70, 'use': 71, 'end': 72, 'amount': 73, 'contact': 74, 'method': 75, 'power': 76, 'add': 77, 'using': 78, 'iteration': 79, 'store': 80, 'code': 81, 'map': 82, 'titled': 83, 'number': 84, 'location': 85, 'result': 86, 'out': 87, 'email': 88, 'counter': 89, 'operation': 90, 'library': 91, 'preferences': 92, 'same': 93, 'string': 94, 'named': 95, 'settings': 96, 'procedure': 97, 'find': 98, 'path': 99, 'learning': 100, 'address': 101, 'int': 102, 'order': 103, 'total': 104, 'an': 105, 'empty': 106, 'subroutine': 107, 'item': 108, 'more': 109, 'condition': 110, 'right': 111, 'start': 112, 'char': 113, 'function': 114, 'xor': 115, 'development': 116, 'sum': 117, 'transform': 118, 'fetch': 119, 'inventory': 120, 'form': 121, 'change': 122, 'net': 123, 'profit': 124, 'current': 125, 'routine': 126, 'partner': 127, 'creation': 128, 'reverse': 129, 'root': 130, 'supplier': 131, 'rate': 132, 'expense': 133, 'double': 134, 'float': 135, 'visible': 136, 'logged': 137, 'account': 138, 'sort': 139, 'insert': 140, 'put': 141, 'carry': 142, 'level': 143, 'feature': 144, 'integer': 145, 'city': 146, 'category': 147, 'sibling': 148, 'divide': 149, 'table': 150, 'include': 151, 'plus': 152, 'uaer': 153, 'their': 154, 'deleted': 155, 'position': 156, 'customer': 157, 'age': 158, 'secondary': 159, 'completed': 160, 'colleague': 161, 'balance': 162, 'stock': 163, 'square': 164, 'imports': 165, 'verified': 166, 'budget': 167, 'report': 168, 'j': 169, 'country': 170, 'system': 171, 'consisting': 172, 'queue': 173, 'stamp': 174, 'schedule': 175, 'has': 176, 'request': 177, 'zip': 178, 'apply': 179, 'cast': 180, 'display': 181, 'elapsed': 182, 'flags': 183, 'allocation': 184, 'mentor': 185, 'bool': 186, 'region': 187, 'transaction': 188, 'price': 189, 'max': 190, 'street': 191, 'i': 192, 'multiply': 193, 'quantity': 194, 'left': 195, 'modulus': 196, 'collection': 197, 'send': 198, 'print': 199, 'approved': 200, 'file': 201, 'programming': 202, 'delivery': 203, 'active': 204, 'character': 205, 'info': 206, 'office': 207, 'reorder': 208, 'point': 209, 'requires': 210, 'gender': 211, 'score': 212, 'identify': 213, 'clear': 214, 'compute': 215, 'validate': 216, 'models': 217, 'boolean': 218, 'due': 219, 'software': 220, 'app': 221, 'k': 222, 'properties': 223, 'subtract': 224, 'void': 225, 'average': 226, 'default': 227, 'options': 228, 'temp': 229, 'tax': 230, 'false': 231, 'banana': 232, 'emergency': 233, 'login': 234, 'last': 235, 'updated': 236, 'employee': 237, 'record': 238, 'guardian': 239, 'containing': 240, 'pos': 241, 'accepts': 242, 'coordinates': 243, 'session': 244, 'discount': 245, 'min': 246, 'output': 247, 'true': 248, 'primary': 249, 'apple': 250, 'cherry': 251, 'gross': 252, 'income': 253, 'supervised': 254, 'alternate': 255, 'rayleigh': 256, 'quotient': 257, 'update': 258, 'qr': 259, 'debit': 260, 'call': 261, 'geo': 262, 'status': 263, 'role': 264, 'access': 265, 'generate': 266, 'push': 267, 'work': 268, 'calculate': 269, 'convert': 270, 'verify': 271, 'assert': 272, 'ensure': 273, 'algorithms': 274, 'description': 275, 'front': 276, 'test': 277, 'deep': 278, 'enabled': 279, 'continuous': 280, 'integration': 281, 'building': 282, 'e': 283, 'householder': 284, 'inverse': 285, 'including': 286, 'givens': 287, 'rotation': 288, 'copy': 289, 'place': 290, 'frame': 291, 'profile': 292, 'items': 293, 'input': 294, 'take': 295, 'do': 296, 'remainder': 297, 'determine': 298, 'import': 299, 'require': 300, 'clustering': 301, 'buffer': 302, 'methods': 303, 'stack': 304, 'framework': 305, 'semi': 306, 'cholesky': 307, 'singular': 308, 'links': 309, 'fields': 310, 'addition': 311, 'element': 312, 'perform': 313, 'receive': 314, 'show': 315, 'back': 316, 'statistical': 317, 'hello': 318, 'javascript': 319, 'management': 320, 'functional': 321, 'this': 322, 'design': 323, 'needs': 324, 'comprising': 325, 'svd': 326, 'eigenvalue': 327, 'lu': 328, 'decomposition': 329, 'errors': 330, 'invoice': 331, 'append': 332, 'values': 333, 'records': 334, 'biggest': 335, 'results': 336, 'orders': 337, 'solve': 338, 'read': 339, 'enter': 340, 'prompt': 341, 'confirm': 342, 'object': 343, 'oriented': 344, 'visualization': 345, 'version': 346, 'control': 347, 'cascading': 348, 'style': 349, 'sheets': 350, 'interface': 351, 'full': 352, 'developer': 353, 'review': 354, 'artificial': 355, 'intelligence': 356, 'revenue': 357, 'having': 358, 'shuffle': 359, 'environment': 360, 'transpose': 361, 'matrix': 362, 'jacobi': 363, 'world': 364, 'hypertext': 365, 'markup': 366, 'language': 367, 'repeat': 368, 'cookie': 369, 'scripts': 370, 'elements': 371, 'vectors': 372, 'aggregate': 373, 'levels': 374, 'size': 375, 'credit': 376, 'limit': 377, 'evaluate': 378, 'rpcmodule': 379, 'regression': 380, 'car': 381, 'bus': 382, 'train': 383, 'plane': 384, 'bicycle': 385, 'unsupervised': 386, 'dog': 387, 'cat': 388, 'bird': 389, 'fish': 390, 'hamster': 391, 'alice': 392, 'speech': 393, 'recognition': 394, 'engineering': 395, 'machine': 396, 'gram': 397, 'schmidt': 398, 'main': 399, 'takes': 400, 'tree': 401, 'trainer': 402, 'openai': 403, 'gpt': 404, 'unit': 405, 'testing': 406, 'packet': 407, 'player': 408, 'scores': 409, 'routes': 410, 'components': 411, 'label': 412, 'database': 413, 'model': 414, 'greatest': 415, 'reports': 416, 'biases': 417, 'details': 418, 'responses': 419, 'names': 420, 'discounts': 421, 'rid': 422, 'dialog': 423, 'boxes': 424, 'exclude': 425, 'sliders': 426, 'packets': 427, 'shopping': 428, 'cart': 429, 'history': 430, 'game': 431, 'classes': 432, 'functions': 433, 'ask': 434, 'treelib': 435, 'york': 436, 'john': 437, 'doe': 438, 'analysis': 439, 'neural': 440, 'networks': 441, 'network': 442, 'mining': 443, 'project': 444, 'determinant': 445, 'adjoint': 446, 'split': 447, 'merge': 448, 'rank': 449, 'search': 450, 'expiry': 451, 'dropdown': 452, 'portfolio': 453, 'payment': 454, 'python': 455, 'information': 456, 'technology': 457, 'graphical': 458, 'science': 459, 'experience': 460, 'computer': 461, 'vision': 462, 'technical': 463, 'support': 464, 'inventories': 465, 'window': 466, 'structures': 467, 'wishlist': 468, 'icon': 469, 'checkboxes': 470, 'coordinate': 471, 'radiobuttons': 472, 'variables': 473, 'frequencies': 474, 'largest': 475, 'maximum': 476, 'shipment': 477, 'lowest': 478, 'highest': 479, 'minimum': 480, 'features': 481, 'remove': 482, 'epochs': 483, 'endpoint': 484, 'length': 485, 'views': 486, 'text': 487, 'achievement': 488, 'frequency': 489, 'cookies': 490, 'weapon': 491, 'profiles': 492, 'leaderboards': 493, 'connection': 494, 'pool': 495, 'css': 496, 'financial': 497, 'statements': 498, 'products': 499, 'js': 500, 'tokens': 501, 'headers': 502, 'addresses': 503, 'message': 504, 'button': 505, 'labels': 506, 'files': 507, 'samples': 508, 'stats': 509, 'debugging': 510, 'architecture': 511, 'expenses': 512, 'pathfinding': 513, 'localelib': 514, 'sys': 515, 'dataanalysis': 516, 'mlutils': 517, 'lib': 518, 'helpers': 519, 'dhfad': 520, 'gmail': 521, 'com': 522, 'phone': 523, 'v': 524, 'security': 525, 'transformation': 526, 'cofactor': 527, 'eigenvector': 528, 'rotate': 529, 'prime': 530, 'linked': 531, 'view': 532, 'hyperparameter': 533, 'tuner': 534, 'checkout': 535, 'catalog': 536, 'node': 537, 'textbox': 538, 'formhandler': 539, 'gateway': 540, 'handler': 541, 'httprequest': 542, 'protocol': 543, 'binary': 544, 'socket': 545, 'integrator': 546, 'httpresponse': 547, 'checkbox': 548, 'random': 549, 'generator': 550, 'solver': 551, 'authenticator': 552, 'bob': 553, 'foo': 554, 'bar': 555, 'baz': 556, 'templates': 557, 'quest': 558, 'events': 559, 'categories': 560, 'reinforcement': 561, 'investment': 562, 'skills': 563, 'styles': 564, 'points': 565, 'invoices': 566, 'measurements': 567, 'layers': 568, 'bias': 569, 'batches': 570, 'modules': 571, 'weight': 572, 'least': 573, 'weapons': 574, 'entry': 575, 'smallest': 576, 'observations': 577, 'countries': 578, 'requests': 579, 'nodes': 580, 'weights': 581, 'quests': 582, 'response': 583, 'leaderboard': 584, 'ranks': 585, 'numbers': 586, 'route': 587, 'paths': 588, 'enemy': 589, 'types': 590, 'header': 591, 'event': 592, 'handlers': 593, 'skill': 594, 'middleware': 595, 'ip': 596, 'windows': 597, 'revenues': 598, 'accounts': 599, 'dns': 600, 'ratings': 601, 'api': 602, 'endpoints': 603, 'achievements': 604, 'middlewares': 605, 'layout': 606, 'loan': 607, 'cities': 608, 'matrices': 609, 'measurement': 610, 'forms': 611, 'series': 612, 'sessions': 613, 'ports': 614, 'reviews': 615, 'vector': 616, 'qtlib': 617, 'probability': 618, 'graphlib': 619, 'numlib': 620, 'foundation': 621, 'mathutils': 622, 'pytorch': 623, 'excellib': 624, 'django': 625, 'imaplib': 626, 'restclient': 627, 'fileutils': 628, 'decryption': 629, 'neuralnet': 630, 'datacleaner': 631, 'xmlparser': 632, 'sortlib': 633, 'dsautils': 634, 'websocketlib': 635, 'uxtoolkit': 636, 'guimodule': 637, 'datautils': 638, 'common': 639, 'keras': 640, 'httpclient': 641, 'webutils': 642, 'uiutils': 643, 'pandas': 644}\n",
      "Vocabulary Size: 645\n",
      "Shape of Input Sequence (# of examples, longest sequence length): (1459, 19)\n",
      "Sample Input Sequence: [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   6 200  38 301\n",
      " 274]\n"
     ]
    }
   ],
   "source": [
    "# create a tokenizer\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(filters='', oov_token='<unk>')\n",
    "\n",
    "# fit the tokenizer on the corpus -> updates internal vocabulary based on corpus\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "# convert the corpus to sequences of integers -> each word is replaced by its index in the vocabulary for each sentence\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "# pad the sequences to the same length -> add padding tokens to the beginning of each sequence to fit the longest sequence\n",
    "padded_sequences = keras.preprocessing.sequence.pad_sequences(sequences, padding='pre')\n",
    "\n",
    "# get the number of unique words (vocabulary size)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# print the tokenizer properties\n",
    "print('Vocabulary:', tokenizer.word_index)\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Shape of Input Sequence (# of examples, longest sequence length):', padded_sequences.shape)\n",
    "print('Sample Input Sequence:', padded_sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Feature Extraction\n",
    "map intents to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Variable Declaration', 1: 'Function Declaration', 2: 'Class Declaration', 3: 'Assignment Operation', 4: 'Conditional Statement', 5: 'Iterative Statement', 6: 'Array Operation', 7: 'Bitwise Operation', 8: 'Mathematical Operation', 9: 'Membership Operation', 10: 'Casting', 11: 'IO Operation', 12: 'Assertion', 13: 'Libraries', 14: 'File System', 15: 'IDE Operation', 16: 'Comments'}\n",
      "Categorial vector shape: (1459, 17)\n"
     ]
    }
   ],
   "source": [
    "# dictionary that maps each intent to a unique index\n",
    "intent_to_index = {intent: index for index, intent in enumerate(unique_intents)}\n",
    "\n",
    "# list for each sentence mapped to its corresponding intent index \n",
    "corpus_intent_mapped_to_index = [intent_to_index[intent] for intent in corpus_intents]\n",
    "\n",
    "# the number of classes to classify a sentence into\n",
    "number_of_classes = len(intent_to_index)\n",
    "\n",
    "# convert intent_to_index to index_to_intent \n",
    "index_to_intent = {index: intent for intent, index in intent_to_index.items()} \n",
    "\n",
    "print(index_to_intent)\n",
    "\n",
    "# one hot encoding for the intents -> length of each vector is equal to the number of classes\n",
    "# each sequence in the dataset is represented as a one-hot encoded vector that represents the intent of the sequence\n",
    "one_hot_encoded_intents = keras.utils.to_categorical(corpus_intent_mapped_to_index, number_of_classes)\n",
    "\n",
    "print('Categorial vector shape:', one_hot_encoded_intents.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Dimension: 17, Output Dimension: 17\n",
      "WARNING:tensorflow:From c:\\Users\\yazmi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 128)         82560     \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 128)               98816     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 17)                1105      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 190737 (745.07 KB)\n",
      "Trainable params: 190737 (745.07 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# the input is the the padded sequences with the target value being the one-hot encoded intents\n",
    "input_dimenstion = len(unique_intents)\n",
    "\n",
    "# the output is the one-hot encoded intents\n",
    "output_dimenstion = one_hot_encoded_intents.shape[1]\n",
    "\n",
    "print(f\"Input Dimension: {input_dimenstion}, Output Dimension: {output_dimenstion}\")\n",
    "\n",
    "# Model description\n",
    "# The model is a sequential model that consists of:\n",
    "# 1. An embedding layer that converts the input sequences to dense vectors of fixed size\n",
    "# 2. A Bidirectional LSTM layer that processes the input sequences in both directions\n",
    "# 3. A Dense layer with 64 units and ReLU activation function\n",
    "# 4. A Dropout layer with a dropout rate of 0.5\n",
    "# 5. A Dense layer with the output dimension and softmax activation function for multi-class classification\n",
    "\n",
    "# define parameters\n",
    "epochs = 100\n",
    "\n",
    "# the embedding dimension is the size of the vector for which each word is represented\n",
    "# the embedding layer of a neural network, output_dim refers to the size of the dense vectors that the layer will generate for each input token (word). \n",
    "# essentially, it is the number of dimensions in which each word will be represented.\n",
    "embedding_dimension = 128\n",
    "\n",
    "# lstm units\n",
    "lstm_units = 64\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dimension),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(lstm_units, dropout=0.2)),\n",
    "    keras.layers.Dense(lstm_units, activation='relu'),\n",
    "    keras.layers.Dropout(0.4),\n",
    "    keras.layers.Dense(output_dimenstion, activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yazmi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yazmi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yazmi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yazmi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2040a4ea140>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_sequences, one_hot_encoded_intents, epochs=epochs, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "Predicted Intent: Function Declaration\n"
     ]
    }
   ],
   "source": [
    "user_input = input(\"Sentence:\")\n",
    "\n",
    "# actual_intent = input(\"Intent:\")\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences([user_input])\n",
    "\n",
    "test_padded_sequences = keras.preprocessing.sequence.pad_sequences(test_sequences, padding='pre')\n",
    "\n",
    "predictions = model.predict(test_padded_sequences)\n",
    "\n",
    "predicted_intent_index = np.argmax(predictions)\n",
    "\n",
    "predicted_intent = index_to_intent[predicted_intent_index]\n",
    "\n",
    "print(f\"Predicted Intent: {predicted_intent}\")\n",
    "\n",
    "# loss, accuracy = model.evaluate(test_padded_sequences, np.array([actual_intent]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
