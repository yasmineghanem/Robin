{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from Logistic_Regression import *\n",
    "\n",
    "# Download stopwords\n",
    "# nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocessing function\n",
    "    # Convert all to lowercase\n",
    "    # Remove punctuations\n",
    "    # Remove stopwords\n",
    "\n",
    "stop_words = {'a', 'the', 'is', 'it', 'to', 'of', 'in', 'for', 'on', 'with', 'as', 'at', 'by', 'from',\n",
    "                 'an', 'be', 'are', 'was', 'were', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she',\n",
    "                   'we', 'they', 'me', 'him', 'her', 'us', 'them', 'my', 'your', 'his', 'its', 'our', 'their',\n",
    "                     'mine', 'yours', 'his', 'hers', 'ours', 'theirs', 'what', 'which', 'who', 'whom', 'whose',\n",
    "                       'where', 'when', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other',\n",
    "                         'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'too', 'very'}\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    text = ' '.join(word for word in text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict intent of custom input\n",
    "def predict_intent(custom_sentence, model):\n",
    "    processed_sentence = preprocess_text(custom_sentence)\n",
    "    X_custom = vectorizer.transform([processed_sentence])\n",
    "    predicted_intent = model.predict(X_custom)\n",
    "    return predicted_intent[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data & Pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>sentence</th>\n",
       "      <th>processed_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Variable Declaration</td>\n",
       "      <td>make start time as double and initialize 0.000123</td>\n",
       "      <td>make start time as double and initialize 0000123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Variable Declaration</td>\n",
       "      <td>declare min value as integer and value 131313</td>\n",
       "      <td>declare min value as integer and value 131313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Variable Declaration</td>\n",
       "      <td>define settings as boolean and value false</td>\n",
       "      <td>define settings as boolean and value false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Variable Declaration</td>\n",
       "      <td>define y as integer and assign to 12345</td>\n",
       "      <td>define y as integer and assign to 12345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Variable Declaration</td>\n",
       "      <td>initialize k as string and initialize it with ...</td>\n",
       "      <td>initialize k as string and initialize it with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2865</th>\n",
       "      <td>Git Operation</td>\n",
       "      <td>push changes</td>\n",
       "      <td>push changes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2866</th>\n",
       "      <td>Git Operation</td>\n",
       "      <td>stage changes</td>\n",
       "      <td>stage changes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2867</th>\n",
       "      <td>Git Operation</td>\n",
       "      <td>push changes</td>\n",
       "      <td>push changes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2868</th>\n",
       "      <td>Git Operation</td>\n",
       "      <td>stage changes</td>\n",
       "      <td>stage changes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2869</th>\n",
       "      <td>Git Operation</td>\n",
       "      <td>discard changes</td>\n",
       "      <td>discard changes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2870 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    intent                                           sentence   \n",
       "0     Variable Declaration  make start time as double and initialize 0.000123  \\\n",
       "1     Variable Declaration      declare min value as integer and value 131313   \n",
       "2     Variable Declaration         define settings as boolean and value false   \n",
       "3     Variable Declaration            define y as integer and assign to 12345   \n",
       "4     Variable Declaration  initialize k as string and initialize it with ...   \n",
       "...                    ...                                                ...   \n",
       "2865         Git Operation                                       push changes   \n",
       "2866         Git Operation                                      stage changes   \n",
       "2867         Git Operation                                       push changes   \n",
       "2868         Git Operation                                      stage changes   \n",
       "2869         Git Operation                                    discard changes   \n",
       "\n",
       "                                     processed_sentence  \n",
       "0      make start time as double and initialize 0000123  \n",
       "1         declare min value as integer and value 131313  \n",
       "2            define settings as boolean and value false  \n",
       "3               define y as integer and assign to 12345  \n",
       "4     initialize k as string and initialize it with ...  \n",
       "...                                                 ...  \n",
       "2865                                       push changes  \n",
       "2866                                      stage changes  \n",
       "2867                                       push changes  \n",
       "2868                                      stage changes  \n",
       "2869                                    discard changes  \n",
       "\n",
       "[2870 rows x 3 columns]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load JSON file\n",
    "with open('../intent_detection_dataset/final_intents_dataset.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Convert to DataFrame\n",
    "rows = []\n",
    "for intent, sentences in data.items():\n",
    "    for sentence in sentences:\n",
    "        rows.append({'intent': intent, 'sentence': sentence})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Apply preprocessing\n",
    "df['processed_sentence'] = df['sentence'].apply(preprocess_text)\n",
    "\n",
    "#Display the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### TF-IDF ###########\n",
    "\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# X = vectorizer.fit_transform(df['processed_sentence'])\n",
    "# y = df['intent']\n",
    "\n",
    "########### N-Gram ###########\n",
    "# vectorizer = CountVectorizer(ngram_range=(1, 2))  # Using unigrams and bigrams\n",
    "\n",
    "# vectorizer = TfidfVectorizer(ngram_range=(1, 2))  # Using unigrams and bigrams\n",
    "\n",
    "# X = vectorizer.fit_transform(df['processed_sentence'])\n",
    "# y = df['intent']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2870,)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "\n",
    "# Extract N-grams\n",
    "def extract_ngrams(text, n=3):\n",
    "    words = text.split()\n",
    "    ngrams = []\n",
    "    for i in range(1, n + 1):\n",
    "        ngrams += [' '.join(words[j:j+i]) for j in range(len(words) - i + 1)]\n",
    "    return ngrams\n",
    "\n",
    "# Compute term frequencies\n",
    "def compute_term_frequencies(corpus):\n",
    "    term_frequencies = defaultdict(Counter)\n",
    "    for idx, document in enumerate(corpus):\n",
    "        ngrams = extract_ngrams(document)\n",
    "        term_frequencies[idx] = Counter(ngrams)\n",
    "    return term_frequencies\n",
    "\n",
    "# Compute document frequencies\n",
    "def compute_document_frequencies(term_frequencies):\n",
    "    document_frequencies = Counter()\n",
    "    for tf in term_frequencies.values():\n",
    "        for term in tf.keys():\n",
    "            document_frequencies[term] += 1\n",
    "    return document_frequencies\n",
    "\n",
    "# Compute TF-IDF\n",
    "def compute_tfidf(corpus):\n",
    "    term_frequencies = compute_term_frequencies(corpus)\n",
    "    document_frequencies = compute_document_frequencies(term_frequencies)\n",
    "    N = len(corpus)\n",
    "    tfidf = defaultdict(dict)\n",
    "    for idx, tf in term_frequencies.items():\n",
    "        for term, count in tf.items():\n",
    "            tfidf[idx][term] = (count / len(tf)) * math.log(N / (document_frequencies[term] + 1))\n",
    "    return tfidf, document_frequencies\n",
    "\n",
    "# Create TF-IDF matrix\n",
    "def create_tfidf_matrix(tfidf, vocabulary):\n",
    "    tfidf_matrix = np.zeros((len(tfidf), len(vocabulary)))\n",
    "    for doc_idx, term_scores in tfidf.items():\n",
    "        for term, score in term_scores.items():\n",
    "            term_idx = vocabulary.get(term)\n",
    "            if term_idx is not None:\n",
    "                tfidf_matrix[doc_idx, term_idx] = score\n",
    "    return tfidf_matrix\n",
    "\n",
    "# Prepare corpus\n",
    "corpus = df['processed_sentence'].tolist()\n",
    "\n",
    "# Compute TF-IDF scores\n",
    "tfidf, document_frequencies  = compute_tfidf(corpus)\n",
    "\n",
    "# Create vocabulary from all N-grams\n",
    "all_ngrams = set()\n",
    "for tf in tfidf.values():\n",
    "    all_ngrams.update(tf.keys())\n",
    "\n",
    "vocabulary = {term: idx for idx, term in enumerate(all_ngrams)}\n",
    "\n",
    "# Create TF-IDF matrix\n",
    "X = create_tfidf_matrix(tfidf, vocabulary)\n",
    "y = df['intent']\n",
    "\n",
    "# Find non-zero values in x\n",
    "# for i in range(len(X)):\n",
    "#     for j in range(len(X[i])):\n",
    "#         if X[i][j] != 0:\n",
    "#              print(X[i][j])\n",
    "#     break\n",
    "\n",
    "# print(y)\n",
    "\n",
    "#shape of y\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24,)\n"
     ]
    }
   ],
   "source": [
    "# Encode labels = 24\n",
    "labels = df['intent'].unique()\n",
    "# print(labels.shape)\n",
    "label_to_index = {label: idx for idx, label in enumerate(labels)}\n",
    "index_to_label = {idx: label for label, idx in label_to_index.items()}\n",
    "y = np.array([label_to_index[label] for label in y])\n",
    "# for i in range(len(y)):\n",
    "# \tprint(y[i])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_train.shape)\n",
    "# print(X_test.shape)\n",
    "# print(y_train.shape)\n",
    "# print(y_test.shape)\n",
    "\n",
    "# for i in range(len(y)):\n",
    "#     print(y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable Declaration\n",
      "(2870, 24)\n"
     ]
    }
   ],
   "source": [
    "# make y_new that is a one hot encoding vector for labels of each example\n",
    "y_new = np.zeros((len(y), len(labels)))\n",
    "for i in range(len(y)):\n",
    "\ty_new[i][y[i]] = 1\n",
    "\n",
    "\n",
    "# get the one index in y_new[0]\n",
    "for i in range(len(y_new[239])):\n",
    "\tif y_new[239][i] == 1:\n",
    "\t\tprint(index_to_label[i])\n",
    "print (y_new.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_new, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape (2296, 24) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[228], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train a classifier (Logistic Regression)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m lg_model \u001b[38;5;241m=\u001b[39m LogisticRegression()\n\u001b[1;32m----> 3\u001b[0m \u001b[43mlg_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Predict on test data\u001b[39;00m\n\u001b[0;32m      6\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m lg_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\jess\\anaconda3\\envs\\robinenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1196\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1194\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[1;32m-> 1196\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mliblinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaga\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1203\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1204\u001b[0m check_classification_targets(y)\n\u001b[0;32m   1205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
      "File \u001b[1;32mc:\\Users\\jess\\anaconda3\\envs\\robinenv\\lib\\site-packages\\sklearn\\base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 584\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    585\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\jess\\anaconda3\\envs\\robinenv\\lib\\site-packages\\sklearn\\utils\\validation.py:1122\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[0;32m   1106\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1107\u001b[0m     X,\n\u001b[0;32m   1108\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1119\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1120\u001b[0m )\n\u001b[1;32m-> 1122\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43m_check_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_numeric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1124\u001b[0m check_consistent_length(X, y)\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32mc:\\Users\\jess\\anaconda3\\envs\\robinenv\\lib\\site-packages\\sklearn\\utils\\validation.py:1143\u001b[0m, in \u001b[0;36m_check_y\u001b[1;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1142\u001b[0m     estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m-> 1143\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcolumn_or_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1144\u001b[0m     _assert_all_finite(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, estimator_name\u001b[38;5;241m=\u001b[39mestimator_name)\n\u001b[0;32m   1145\u001b[0m     _ensure_no_complex_data(y)\n",
      "File \u001b[1;32mc:\\Users\\jess\\anaconda3\\envs\\robinenv\\lib\\site-packages\\sklearn\\utils\\validation.py:1202\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[1;34m(y, dtype, warn)\u001b[0m\n\u001b[0;32m   1193\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1194\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA column-vector y was passed when a 1d array was\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1195\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected. Please change the shape of y to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1198\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   1199\u001b[0m         )\n\u001b[0;32m   1200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _asarray_with_order(xp\u001b[38;5;241m.\u001b[39mreshape(y, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[1;32m-> 1202\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1203\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my should be a 1d array, got an array of shape \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(shape)\n\u001b[0;32m   1204\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: y should be a 1d array, got an array of shape (2296, 24) instead."
     ]
    }
   ],
   "source": [
    "\n",
    "# Train a classifier (Logistic Regression)\n",
    "lg_model = LogisticRegression()\n",
    "lg_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = lg_model.predict(X_test)\n",
    "\n",
    "# Map index to label\n",
    "y_test_labels = [index_to_label[idx] for idx in y_test]\n",
    "y_pred_labels = [index_to_label[idx] for idx in y_pred]\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test_labels, y_pred_labels))\n",
    "# print(\"Classification Report:\\n\", classification_report(y_test_labels, y_pred_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted intent for 'i want a method named ADD ':\n",
      " Function Declaration\n"
     ]
    }
   ],
   "source": [
    "# Function to predict intent of custom input\n",
    "def predict_intent(custom_sentence):\n",
    "    # Preprocess the custom input\n",
    "    processed_sentence = preprocess_text(custom_sentence)\n",
    "    # Convert to N-gram TF-IDF features\n",
    "    ngrams = extract_ngrams(processed_sentence)\n",
    "    custom_tfidf = np.zeros(len(vocabulary))\n",
    "    term_counts = Counter(ngrams)\n",
    "    N = len(corpus) + 1\n",
    "    for term, count in term_counts.items():\n",
    "        if term in vocabulary:\n",
    "            term_idx = vocabulary[term]\n",
    "            tf = count / len(ngrams)\n",
    "            idf = math.log(N / (1 + document_frequencies.get(term, 0)))\n",
    "            custom_tfidf[term_idx] = tf * idf\n",
    "    # Predict the intent\n",
    "    predicted_intent_idx = lg_model.predict([custom_tfidf])[0]\n",
    "    predicted_intent = index_to_label[predicted_intent_idx]\n",
    "    return predicted_intent\n",
    "\n",
    "# Test the model with custom input\n",
    "custom_sentence = \"i want a method named ADD \"\n",
    "predicted_intent = predict_intent(custom_sentence)\n",
    "print(f\"Predicted intent for '{custom_sentence}':\\n {predicted_intent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 5936 features, but LogisticRegression is expecting 6090 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[129], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Test the model with custom input\u001b[39;00m\n\u001b[0;32m     13\u001b[0m custom_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massign x equals 5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 14\u001b[0m predicted_intent \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_intent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcustom_sentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted intent for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcustom_sentence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_intent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[129], line 8\u001b[0m, in \u001b[0;36mpredict_intent\u001b[1;34m(custom_sentence)\u001b[0m\n\u001b[0;32m      6\u001b[0m X_custom \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform([processed_sentence])\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Predict the intent\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m predicted_intent_idx \u001b[38;5;241m=\u001b[39m \u001b[43mlg_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_custom\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      9\u001b[0m predicted_intent \u001b[38;5;241m=\u001b[39m index_to_label[predicted_intent_idx]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predicted_intent\n",
      "File \u001b[1;32mc:\\Users\\jess\\anaconda3\\envs\\robinenv\\lib\\site-packages\\sklearn\\linear_model\\_base.py:419\u001b[0m, in \u001b[0;36mLinearClassifierMixin.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;124;03mPredict class labels for samples in X.\u001b[39;00m\n\u001b[0;32m    407\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;124;03m    Vector containing the class labels for each sample.\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    418\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 419\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(scores\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    421\u001b[0m     indices \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(scores \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jess\\anaconda3\\envs\\robinenv\\lib\\site-packages\\sklearn\\linear_model\\_base.py:400\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    397\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    398\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 400\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    401\u001b[0m scores \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39mreshape(scores, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m scores\n",
      "File \u001b[1;32mc:\\Users\\jess\\anaconda3\\envs\\robinenv\\lib\\site-packages\\sklearn\\base.py:588\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    585\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 588\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\jess\\anaconda3\\envs\\robinenv\\lib\\site-packages\\sklearn\\base.py:389\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 389\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    390\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    391\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    392\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 5936 features, but LogisticRegression is expecting 6090 features as input."
     ]
    }
   ],
   "source": [
    "# # Function to predict intent of custom input\n",
    "# def predict_intent(custom_sentence):\n",
    "#     # Preprocess the custom input\n",
    "#     processed_sentence = preprocess_text(custom_sentence)\n",
    "#     # Convert to N-gram features\n",
    "#     X_custom = vectorizer.transform([processed_sentence])\n",
    "#     # Predict the intent\n",
    "#     predicted_intent_idx = lg_model.predict(X_custom)[0]\n",
    "#     predicted_intent = index_to_label[predicted_intent_idx]\n",
    "#     return predicted_intent\n",
    "\n",
    "# # Test the model with custom input\n",
    "# custom_sentence = \"assign x equals 5\"\n",
    "# predicted_intent = predict_intent(custom_sentence)\n",
    "# print(f\"Predicted intent for '{custom_sentence}':\\n {predicted_intent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted intent for 'i want a method called predict entity': \n",
      "IDE Operation\n"
     ]
    }
   ],
   "source": [
    "# Test the model with custom input\n",
    "custom_sentence = \"i want a method called predict entity\"\n",
    "predicted_intent = predict_intent(custom_sentence, lg_model)\n",
    "print(f\"Predicted intent for '{custom_sentence}': \\n{predicted_intent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.980836236933798\n",
      "Classification Report:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "  Activate Interactive       1.00      1.00      1.00         6\n",
      "        Activate Mouse       1.00      1.00      1.00         7\n",
      "       Array Operation       1.00      0.86      0.93        22\n",
      "             Assertion       1.00      0.83      0.91         6\n",
      "  Assignment Operation       0.90      0.75      0.82        12\n",
      "     Bitwise Operation       1.00      1.00      1.00        20\n",
      "               Casting       0.94      1.00      0.97        15\n",
      "     Class Declaration       1.00      1.00      1.00         8\n",
      "               Comment       1.00      0.82      0.90        11\n",
      " Conditional Operation       0.97      0.97      0.97        36\n",
      "  Constant Declaration       1.00      1.00      1.00        46\n",
      "           File System       1.00      0.96      0.98        25\n",
      "              For Loop       0.95      1.00      0.98        21\n",
      "  Function Declaration       1.00      1.00      1.00        23\n",
      "         Git Operation       1.00      1.00      1.00        17\n",
      "         IDE Operation       1.00      1.00      1.00        53\n",
      "                 Input       1.00      1.00      1.00        11\n",
      "  Interactive Commands       1.00      1.00      1.00        44\n",
      "             Libraries       1.00      1.00      1.00         4\n",
      "Mathematical Operation       0.92      1.00      0.96        77\n",
      "  Membership Operation       1.00      1.00      1.00        33\n",
      "                Output       1.00      1.00      1.00        17\n",
      "  Variable Declaration       1.00      1.00      1.00        48\n",
      "            While Loop       1.00      1.00      1.00        12\n",
      "\n",
      "              accuracy                           0.98       574\n",
      "             macro avg       0.99      0.97      0.98       574\n",
      "          weighted avg       0.98      0.98      0.98       574\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "# Train the model\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted intent for 'i want a function called predict': \n",
      "Mathematical Operation\n"
     ]
    }
   ],
   "source": [
    "# Test the model with custom input\n",
    "#declare constant x as string\n",
    "#i want to declare a function\n",
    "custom_sentence = \"i want a function called predict\"\n",
    "predicted_intent = predict_intent(custom_sentence, model)\n",
    "print(f\"Predicted intent for '{custom_sentence}': \\n{predicted_intent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2870, 574]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[170], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m classRep \u001b[38;5;241m=\u001b[39m \u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(classRep)\n",
      "File \u001b[1;32mc:\\Users\\jess\\anaconda3\\envs\\robinenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2310\u001b[0m, in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   2195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclassification_report\u001b[39m(\n\u001b[0;32m   2196\u001b[0m     y_true,\n\u001b[0;32m   2197\u001b[0m     y_pred,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2205\u001b[0m ):\n\u001b[0;32m   2206\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a text report showing the main classification metrics.\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m \n\u001b[0;32m   2208\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <classification_report>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2307\u001b[0m \u001b[38;5;124;03m    <BLANKLINE>\u001b[39;00m\n\u001b[0;32m   2308\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2310\u001b[0m     y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2312\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2313\u001b[0m         labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\n",
      "File \u001b[1;32mc:\\Users\\jess\\anaconda3\\envs\\robinenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:86\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     60\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     88\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jess\\anaconda3\\envs\\robinenv\\lib\\site-packages\\sklearn\\utils\\validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    395\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    398\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    399\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    400\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [2870, 574]"
     ]
    }
   ],
   "source": [
    "classRep = classification_report(y,y_pred)\n",
    "print(classRep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (574,6148) and (6147,24) not aligned: 6148 (dim 1) != 6147 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[169], line 148\u001b[0m\n\u001b[0;32m    145\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# Predictions\u001b[39;00m\n\u001b[1;32m--> 148\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m y_pred_labels \u001b[38;5;241m=\u001b[39m [index_to_label[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m y_pred]\n\u001b[0;32m    150\u001b[0m y_test_labels \u001b[38;5;241m=\u001b[39m [index_to_label[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m y_test]\n",
      "Cell \u001b[1;32mIn[169], line 137\u001b[0m, in \u001b[0;36mCustomLogisticRegression.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    136\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__add_intercept(X)\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[169], line 133\u001b[0m, in \u001b[0;36mCustomLogisticRegression.predict_prob\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_intercept:\n\u001b[0;32m    131\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__add_intercept(X)\n\u001b[1;32m--> 133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__sigmoid(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (574,6148) and (6147,24) not aligned: 6148 (dim 1) != 6147 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Convert JSON to DataFrame\n",
    "rows = []\n",
    "for intent, sentences in data.items():\n",
    "    for sentence in sentences:\n",
    "        rows.append({'intent': intent, 'sentence': sentence})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "df['processed_sentence'] = df['sentence'].apply(preprocess_text)\n",
    "\n",
    "# Extract N-grams\n",
    "def extract_ngrams(text, n=2):\n",
    "    words = text.split()\n",
    "    ngrams = []\n",
    "    for i in range(1, n + 1):\n",
    "        ngrams += [' '.join(words[j:j+i]) for j in range(len(words) - i + 1)]\n",
    "    return ngrams\n",
    "\n",
    "# Compute term frequencies\n",
    "def compute_term_frequencies(corpus):\n",
    "    term_frequencies = defaultdict(Counter)\n",
    "    for idx, document in enumerate(corpus):\n",
    "        ngrams = extract_ngrams(document)\n",
    "        term_frequencies[idx] = Counter(ngrams)\n",
    "    return term_frequencies\n",
    "\n",
    "# Compute document frequencies\n",
    "def compute_document_frequencies(term_frequencies):\n",
    "    document_frequencies = Counter()\n",
    "    for tf in term_frequencies.values():\n",
    "        for term in tf.keys():\n",
    "            document_frequencies[term] += 1\n",
    "    return document_frequencies\n",
    "\n",
    "# Compute TF-IDF\n",
    "def compute_tfidf(corpus):\n",
    "    term_frequencies = compute_term_frequencies(corpus)\n",
    "    document_frequencies = compute_document_frequencies(term_frequencies)\n",
    "    N = len(corpus)\n",
    "    tfidf = defaultdict(dict)\n",
    "    for idx, tf in term_frequencies.items():\n",
    "        for term, count in tf.items():\n",
    "            tfidf[idx][term] = (count / len(tf)) * math.log(N / (document_frequencies[term] + 1))\n",
    "    return tfidf, document_frequencies\n",
    "\n",
    "# Create TF-IDF matrix\n",
    "def create_tfidf_matrix(tfidf, vocabulary):\n",
    "    tfidf_matrix = np.zeros((len(tfidf), len(vocabulary)))\n",
    "    for doc_idx, term_scores in tfidf.items():\n",
    "        for term, score in term_scores.items():\n",
    "            term_idx = vocabulary.get(term)\n",
    "            if term_idx is not None:\n",
    "                tfidf_matrix[doc_idx, term_idx] = score\n",
    "    return tfidf_matrix\n",
    "\n",
    "# Prepare corpus\n",
    "corpus = df['processed_sentence'].tolist()\n",
    "\n",
    "# Compute TF-IDF scores\n",
    "tfidf, document_frequencies = compute_tfidf(corpus)\n",
    "\n",
    "# Create vocabulary from all N-grams\n",
    "all_ngrams = set()\n",
    "for tf in tfidf.values():\n",
    "    all_ngrams.update(tf.keys())\n",
    "\n",
    "vocabulary = {term: idx for idx, term in enumerate(all_ngrams)}\n",
    "\n",
    "# Create TF-IDF matrix\n",
    "X = create_tfidf_matrix(tfidf, vocabulary)\n",
    "y = df['intent']\n",
    "\n",
    "# Encode labels\n",
    "labels = df['intent'].unique()\n",
    "label_to_index = {label: idx for idx, label in enumerate(labels)}\n",
    "index_to_label = {idx: label for label, idx in label_to_index.items()}\n",
    "y = np.array([label_to_index[label] for label in y])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Implementing Logistic Regression\n",
    "class CustomLogisticRegression:\n",
    "    def __init__(self, lr=0.01, num_iter=1000, fit_intercept=True, verbose=False):\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def __loss(self, h, y):\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "        self.theta = np.zeros((X.shape[1], len(np.unique(y))))\n",
    "        \n",
    "        for i in range(self.num_iter):\n",
    "            z = np.dot(X, self.theta)\n",
    "            h = self.__sigmoid(z)\n",
    "            gradient = np.dot(X.T, (h - self.__one_hot_encode(y))) / y.size\n",
    "            self.theta -= self.lr * gradient\n",
    "            \n",
    "            if(self.verbose == True and i % 100 == 0):\n",
    "                z = np.dot(X, self.theta)\n",
    "                h = self.__sigmoid(z)\n",
    "                print(f'Loss: {self.__loss(h, y)} \\t')\n",
    "    \n",
    "    def predict_prob(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "    \n",
    "        return self.__sigmoid(np.dot(X, self.theta))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = self.__add_intercept(X)\n",
    "        return np.argmax(self.predict_prob(X), axis=1)\n",
    "    \n",
    "    def __one_hot_encode(self, y):\n",
    "        n_values = np.max(y) + 1\n",
    "        return np.eye(n_values)[y]\n",
    "\n",
    "# Train the logistic regression model\n",
    "model = CustomLogisticRegression(lr=0.1, num_iter=3000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_labels = [index_to_label[idx] for idx in y_pred]\n",
    "y_test_labels = [index_to_label[idx] for idx in y_test]\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test_labels, y_pred_labels))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_labels, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.08013937282229965\n",
      "Classification Report:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "  Activate Interactive       0.00      0.00      0.00         6\n",
      "        Activate Mouse       0.00      0.00      0.00         7\n",
      "       Array Operation       0.00      0.00      0.00        22\n",
      "             Assertion       0.00      0.00      0.00         6\n",
      "  Assignment Operation       0.00      0.00      0.00        12\n",
      "     Bitwise Operation       0.00      0.00      0.00        20\n",
      "               Casting       0.00      0.00      0.00        15\n",
      "     Class Declaration       0.00      0.00      0.00         8\n",
      "               Comment       0.00      0.00      0.00        11\n",
      " Conditional Operation       0.00      0.00      0.00        36\n",
      "  Constant Declaration       0.08      1.00      0.15        46\n",
      "           File System       0.00      0.00      0.00        25\n",
      "              For Loop       0.00      0.00      0.00        21\n",
      "  Function Declaration       0.00      0.00      0.00        23\n",
      "         Git Operation       0.00      0.00      0.00        17\n",
      "         IDE Operation       0.00      0.00      0.00        53\n",
      "                 Input       0.00      0.00      0.00        11\n",
      "  Interactive Commands       0.00      0.00      0.00        44\n",
      "             Libraries       0.00      0.00      0.00         4\n",
      "Mathematical Operation       0.00      0.00      0.00        77\n",
      "  Membership Operation       0.00      0.00      0.00        33\n",
      "                Output       0.00      0.00      0.00        17\n",
      "  Variable Declaration       0.00      0.00      0.00        48\n",
      "            While Loop       0.00      0.00      0.00        12\n",
      "\n",
      "              accuracy                           0.08       574\n",
      "             macro avg       0.00      0.04      0.01       574\n",
      "          weighted avg       0.01      0.08      0.01       574\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jess\\anaconda3\\envs\\robinenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\jess\\anaconda3\\envs\\robinenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\jess\\anaconda3\\envs\\robinenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[157], line 81\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Test the model with custom input\u001b[39;00m\n\u001b[0;32m     80\u001b[0m custom_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi want a method named ADD \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 81\u001b[0m predicted_intent \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_intent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcustom_sentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted intent for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcustom_sentence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_intent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[157], line 75\u001b[0m, in \u001b[0;36mpredict_intent\u001b[1;34m(custom_sentence)\u001b[0m\n\u001b[0;32m     73\u001b[0m         custom_tfidf[term_idx] \u001b[38;5;241m=\u001b[39m tf \u001b[38;5;241m*\u001b[39m idf\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Predict the intent\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m predicted_intent_idx \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcustom_tfidf\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     76\u001b[0m predicted_intent \u001b[38;5;241m=\u001b[39m index_to_label[predicted_intent_idx]\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predicted_intent\n",
      "Cell \u001b[1;32mIn[157], line 44\u001b[0m, in \u001b[0;36mLogisticRegressionModel.predict\u001b[1;34m(self, X, threshold)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m):\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m threshold\n",
      "Cell \u001b[1;32mIn[157], line 39\u001b[0m, in \u001b[0;36mLogisticRegressionModel.predict_prob\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_intercept:\n\u001b[1;32m---> 39\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__add_intercept\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__sigmoid(np\u001b[38;5;241m.\u001b[39mdot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtheta))\n",
      "Cell \u001b[1;32mIn[157], line 10\u001b[0m, in \u001b[0;36mLogisticRegressionModel.__add_intercept\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__add_intercept\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m---> 10\u001b[0m     intercept \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mconcatenate((intercept, X), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# Implementing Logistic Regression\n",
    "class LogisticRegressionModel:\n",
    "    def __init__(self, lr=0.01, num_iter=1000, fit_intercept=True, verbose=False):\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def __loss(self, h, y):\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "        # weights initialization\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        \n",
    "        for i in range(self.num_iter):\n",
    "            z = np.dot(X, self.theta)\n",
    "            h = self.__sigmoid(z)\n",
    "            gradient = np.dot(X.T, (h - y)) / y.size\n",
    "            self.theta -= self.lr * gradient\n",
    "            \n",
    "            if(self.verbose == True and i % 100 == 0):\n",
    "                z = np.dot(X, self.theta)\n",
    "                h = self.__sigmoid(z)\n",
    "                print(f'Loss: {self.__loss(h, y)} \\t')\n",
    "    \n",
    "    def predict_prob(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "    \n",
    "        return self.__sigmoid(np.dot(X, self.theta))\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return self.predict_prob(X) >= threshold\n",
    "\n",
    "# Train the logistic regression model\n",
    "model = LogisticRegressionModel(lr=0.1, num_iter=3000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_labels = [index_to_label[idx] for idx in y_pred]\n",
    "y_test_labels = [index_to_label[idx] for idx in y_test]\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test_labels, y_pred_labels))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_labels, y_pred_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted intent for 'import np as numpy':\n",
      " Constant Declaration\n"
     ]
    }
   ],
   "source": [
    "def predict_intent(custom_sentence, model, document_frequencies, vocabulary, N):\n",
    "    # Preprocess the custom input\n",
    "    processed_sentence = preprocess_text(custom_sentence)\n",
    "    # Convert to N-gram TF-IDF features\n",
    "    ngrams = extract_ngrams(processed_sentence)\n",
    "    custom_tfidf = np.zeros(len(vocabulary))\n",
    "    term_counts = Counter(ngrams)\n",
    "    for term, count in term_counts.items():\n",
    "        if term in vocabulary:\n",
    "            term_idx = vocabulary[term]\n",
    "            tf = count / len(ngrams)\n",
    "            idf = math.log(N / (1 + document_frequencies.get(term, 0)))\n",
    "            custom_tfidf[term_idx] = tf * idf\n",
    "    # Reshape custom_tfidf to be a 2D array\n",
    "    custom_tfidf = custom_tfidf.reshape(1, -1)\n",
    "    # Predict the intent\n",
    "    predicted_intent_idx = model.predict(custom_tfidf)[0]\n",
    "    predicted_intent = index_to_label[predicted_intent_idx]\n",
    "    return predicted_intent\n",
    "\n",
    "# Test the model with custom input\n",
    "custom_sentence = \"import np as numpy\"\n",
    "predicted_intent = predict_intent(custom_sentence, model, document_frequencies, vocabulary, len(corpus) + 1)\n",
    "print(f\"Predicted intent for '{custom_sentence}':\\n {predicted_intent}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robinenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
