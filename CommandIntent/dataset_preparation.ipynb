{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from string import Formatter\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def augment_intent(file_path, target_intent, number_of_augmentations=10):\n",
    "    '''\n",
    "    This function augments the data in the dataset\n",
    "\n",
    "    It take the intent keywords and augments them using the synonym augmenter and inserts spelling mistakes in the keywords and saves the augmented data to a new file\n",
    "\n",
    "    Args:\n",
    "        - file_path (str) : path of the dataset\n",
    "    '''\n",
    "\n",
    "    with open(file_path, 'r') as intents_file:\n",
    "        data = json.load(intents_file)\n",
    "\n",
    "        intents = data[\"intents\"]\n",
    "\n",
    "        for intent in intents:    \n",
    "            if intent[\"intent\"] == target_intent:\n",
    "                print(intent[\"intent\"])\n",
    "                    \n",
    "                # intent_data = intent[target_intent]\n",
    "\n",
    "                default_parameters = intent[\"default parameters\"]\n",
    "\n",
    "                templates = intent[\"patterns\"]\n",
    "\n",
    "                formatted_templates = []\n",
    "\n",
    "                print(len(templates))\n",
    "\n",
    "                for template in templates:\n",
    "\n",
    "                    number = number_of_augmentations\n",
    "\n",
    "                    for _ in range(number):\n",
    "\n",
    "                        filtered_parameters = {\n",
    "                            field_name for _, field_name, _, _ in Formatter().parse(template) if field_name}\n",
    "\n",
    "                        final_parameters = {\n",
    "                            key: default_parameters[key] for key in filtered_parameters}\n",
    "\n",
    "                        if intent[\"intent\"] == \"Variable Declaration\":\n",
    "                            if \"datatype\" in final_parameters:\n",
    "\n",
    "                                # we check if the type is in the template then we check the type to choose the appropriate synonyms\n",
    "                                final_parameters[\"datatype\"] = random.choice(\n",
    "                                    intent[\"synonyms\"][\"datatype\"])\n",
    "\n",
    "                                match final_parameters[\"datatype\"]:\n",
    "                                    case \"int\" | \"integer\":\n",
    "                                        final_parameters[\"value\"] = random.choice(\n",
    "                                            intent[\"synonyms\"][\"value\"][\"integer\"])\n",
    "                                    case \"float\" | \"double\":\n",
    "                                        final_parameters[\"value\"] = random.choice(\n",
    "                                            intent[\"synonyms\"][\"value\"][\"float\"])\n",
    "                                    case \"string\":\n",
    "                                        final_parameters[\"value\"] = random.choice(\n",
    "                                            intent[\"synonyms\"][\"value\"][\"string\"])\n",
    "                                    case \"char\" | \"character\":\n",
    "                                        final_parameters[\"value\"] = chr(\n",
    "                                            random.randint(32, 123))\n",
    "                                    case \"bool\" | \"boolean\":\n",
    "                                        final_parameters[\"value\"] = random.choice(\n",
    "                                            intent[\"synonyms\"][\"value\"][\"boolean\"])\n",
    "                            else:\n",
    "                                final_parameters[\"value\"] = random.choice(\n",
    "                                    list(itertools.chain(*intent[\"synonyms\"][\"value\"].values())))\n",
    "\n",
    "                        for parameter in final_parameters:\n",
    "\n",
    "                            if intent[\"intent\"] == \"Variable Declaration\":\n",
    "                                if parameter == \"value\" or parameter == \"datatype\":\n",
    "                                    continue\n",
    "                            if parameter == \"value\":\n",
    "                                if type(intent[\"synonyms\"][\"value\"]) == dict:\n",
    "                                    final_parameters[\"value\"] = random.choice(\n",
    "                                        list(itertools.chain(*intent[\"synonyms\"][\"value\"].values())))\n",
    "                                    continue\n",
    "                            \n",
    "                            if parameter == \"variable_1\" or parameter == \"variable_2\" or parameter == \"variable_3\":\n",
    "                                final_parameters[parameter] = random.choice(\n",
    "                                    intent[\"synonyms\"][\"variable\"])\n",
    "                                continue\n",
    "                            \n",
    "                            if parameter == \"start\" or parameter == \"end\" or parameter == \"step\":\n",
    "                                final_parameters[parameter] = random.choice(intent[\"synonyms\"][\"number\"])\n",
    "                                continue\n",
    "\n",
    "                            synonyms = intent[\"synonyms\"].get(parameter, [])\n",
    "                            final_parameters[parameter] = random.choice(synonyms)\n",
    "\n",
    "                        formatted_string = template.format(**final_parameters)\n",
    "\n",
    "                        formatted_templates.append(formatted_string)\n",
    "\n",
    "                intent[\"formatted patterns\"] = formatted_templates\n",
    "\n",
    "                print(len(formatted_templates))\n",
    "\n",
    "                with open(file_path, 'w') as file:\n",
    "                    json.dump(data, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(file_path, intents=['all'], number_of_augmentations=10):\n",
    "    if intents == ['all']:\n",
    "        with open(file_path, 'r') as intents_file:\n",
    "            data = json.load(intents_file)\n",
    "\n",
    "            intents = data[\"intents\"]\n",
    "            for intent in intents:\n",
    "                augment_intent(file_path, intent, number_of_augmentations)\n",
    "    else:\n",
    "        for intent in intents:\n",
    "            augment_intent(file_path, intent, number_of_augmentations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDE Operation\n",
      "30\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "intents_to_augment = [\"IDE Operation\"]\n",
    "augment_data(\"./intent_detection_dataset/more_intents_pattern.json\", intents_to_augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.ner_dataset_pre_annotations(\"./intent_detection_dataset/more_intents_pattern.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(\"./ner_dataset/annotations/final_annotations\"):\n",
    "    utils.reformat_json(f\"./ner_dataset/annotations/final_annotations/{file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the annotations of the data to the annotations file\n",
    "with open(\"./ner_dataset/annotations/annotations.json\") as f:\n",
    "    annotations = json.load(f)\n",
    "    \n",
    "    for file in os.listdir(\"./ner_dataset/annotations/final_annotations\"):\n",
    "\n",
    "        with open(f\"./ner_dataset/annotations/final_annotations/{file}\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "\n",
    "        intent = re.sub(r\"\\.json\", \"\", file)\n",
    "        intent = re.sub(r\"_\", \" \", intent)\n",
    "\n",
    "        annotations[\"annotations\"][intent] = data[\"annotations\"]\n",
    "        \n",
    "        with open(\"./ner_dataset/annotations/annotations.json\", \"w\") as f:\n",
    "            json.dump(annotations, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.reformat_json(\"./ner_dataset/annotations/annotations.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_subset_indices(subset, larger_list):\n",
    "    indices = []\n",
    "    for item in subset:\n",
    "        try:\n",
    "            # Find the index of the current item in the larger list\n",
    "            index = larger_list.index(item)\n",
    "            indices.append(index)\n",
    "        except ValueError:\n",
    "            # If an item is not found, return None indicating subset is not fully present\n",
    "            return None\n",
    "    return indices\n",
    "\n",
    "def remove_punctuation(input_string):\n",
    "    # Define a regex pattern to match punctuation (excluding underscore and digits)\n",
    "    # exclude special characters if present alone\n",
    "    \n",
    "    pattern = r'(?<!\\d)\\.(?![\\d\\s])|[^\\w\\s.\\-]|_'\n",
    "\n",
    "    # Use re.sub to substitute all matches of the pattern with an empty string\n",
    "    return re.sub(pattern, '', input_string)\n",
    "\n",
    "def find_subset_indices(subset, larger_list):\n",
    "    indices = []\n",
    "    for item in subset:\n",
    "        try:\n",
    "            # Find the index of the current item in the larger list\n",
    "            index = larger_list.index(item)\n",
    "            indices.append(index)\n",
    "        except ValueError:\n",
    "            # If an item is not found, return None indicating subset is not fully present\n",
    "            return None\n",
    "    return indices\n",
    "\n",
    "def convert_annotations_to_csv(file_path):\n",
    "\n",
    "    # delete the csv file if it exists\n",
    "    if os.path.exists(\"./ner_dataset/ner_dataset.csv\"):\n",
    "        os.remove(\"./ner_dataset/ner_dataset.csv\")\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "        annotations = data['annotations']\n",
    "\n",
    "        intents = annotations.keys()\n",
    "        print(intents)\n",
    "\n",
    "        with open(\"./ner_dataset/ner_dataset.csv\", 'w') as csv_file:\n",
    "            csv_file.write(\"Sentence #, Word, Tag, Intent\\n\")\n",
    "\n",
    "            sentence_index = 0\n",
    "\n",
    "            for intent in intents: \n",
    "                print(intent)\n",
    "                print(len(annotations))\n",
    "                examples = annotations[intent]\n",
    "                print(len(examples))\n",
    "                if len(examples) == 0:\n",
    "                    continue\n",
    "                \n",
    "                for example in examples:\n",
    "                    # if intent == \"io operation\":\n",
    "                    #     print(example)\n",
    "\n",
    "                    if example is None:\n",
    "                        continue\n",
    "                        \n",
    "                    sentence = example[0]\n",
    "                    # print(sentence)\n",
    "                    entities = example[1][\"entities\"]\n",
    "                    # print(entities)\n",
    "                    # print(sentence_index)\n",
    "                    # print(sentence)\n",
    "                    # print(entities)\n",
    "\n",
    "                    sentence = sentence[:-1]\n",
    "                    \n",
    "                    # if intent == \"comment\":\n",
    "                    #     print(sentence)\n",
    "\n",
    "                    if len(sentence) == 0:\n",
    "                        continue\n",
    "\n",
    "                    if sentence[-1] == \".\":\n",
    "                        sentence = sentence[:-1]\n",
    "\n",
    "                    # print(sentence)\n",
    "                    words = sentence.split(\" \")\n",
    "\n",
    "                    tags = [\"O\"] * len(words)\n",
    "\n",
    "                    words = [remove_punctuation(word) for word in words]\n",
    "\n",
    "                    # print(words)\n",
    "                    # print(tags)\n",
    "\n",
    "                    for entity in entities:\n",
    "                        start = entity[0]\n",
    "                        end = entity[1]\n",
    "                        tag = entity[2]\n",
    "                        split_entity = sentence[start:end].split(\" \")\n",
    "                        # print(split_entity)\n",
    "\n",
    "                        indices = find_subset_indices(split_entity, words)\n",
    "\n",
    "                        # print(indices)\n",
    "                        # print(indices)\n",
    "\n",
    "                        if indices is not None:\n",
    "                            indices.sort()\n",
    "                            for i, index in enumerate(indices):\n",
    "                                if i == 0:\n",
    "                                    tags[index] = f\"B-{tag}\"\n",
    "                                else:\n",
    "                                    tags[index] = f\"I-{tag}\"\n",
    "                        \n",
    "                        # print(tags)\n",
    "                        # break\n",
    "                    \n",
    "                    for word, tag in zip(words, tags):\n",
    "                        csv_file.write(f\"{sentence_index}, {word}, {tag}, {intent}\\n\")\n",
    "\n",
    "                    sentence_index += 1\n",
    "\n",
    "                    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['assertion', 'assignment operation', 'bitwise operation', 'casting', 'class declaration', 'comment', 'conditional operation', 'constant declaration', 'file system', 'for loop', 'function declaration', 'git operation', 'ide operation', 'input', 'interactive commands', 'libraries', 'mathematical operation', 'membership operation', 'output', 'variable declaration', 'while loop'])\n",
      "assertion\n",
      "21\n",
      "49\n",
      "assignment operation\n",
      "21\n",
      "80\n",
      "bitwise operation\n",
      "21\n",
      "97\n",
      "casting\n",
      "21\n",
      "78\n",
      "class declaration\n",
      "21\n",
      "38\n",
      "comment\n",
      "21\n",
      "50\n",
      "conditional operation\n",
      "21\n",
      "160\n",
      "constant declaration\n",
      "21\n",
      "200\n",
      "file system\n",
      "21\n",
      "150\n",
      "for loop\n",
      "21\n",
      "120\n",
      "function declaration\n",
      "21\n",
      "112\n",
      "git operation\n",
      "21\n",
      "80\n",
      "ide operation\n",
      "21\n",
      "290\n",
      "input\n",
      "21\n",
      "50\n",
      "interactive commands\n",
      "21\n",
      "220\n",
      "libraries\n",
      "21\n",
      "51\n",
      "mathematical operation\n",
      "21\n",
      "370\n",
      "membership operation\n",
      "21\n",
      "114\n",
      "output\n",
      "21\n",
      "120\n",
      "variable declaration\n",
      "21\n",
      "240\n",
      "while loop\n",
      "21\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "convert_annotations_to_csv(\"./ner_dataset/annotations/annotations.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create file for final intents dataset\n",
    "with open(\"./intent_detection_dataset/final_intents_dataset.json\", \"w\") as f_final:\n",
    "    final_data = {}\n",
    "\n",
    "    with open(\"./intent_detection_dataset/more_intents_pattern.json\", \"r\") as f_intents:\n",
    "        intents = json.load(f_intents)['intents']\n",
    "        \n",
    "        for intent in intents:\n",
    "            intent_name = intent[\"intent\"]\n",
    "            final_patterns = intent[\"formatted patterns\"]\n",
    "            final_data[intent_name] = final_patterns\n",
    "\n",
    "    json.dump(final_data, f_final, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 7), match='123.456'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "positive_float_pattern = r'^\\d*\\.\\d+$'\n",
    "print(re.match(positive_float_pattern, '123.456'))  # Match\n",
    "print(re.match(positive_float_pattern, '-123.456'))  # No match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 2, 3)]\n"
     ]
    }
   ],
   "source": [
    "print(list(zip([1], [2], [3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yazmi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms of push: {'pushing', 'crowd', 'press', 'energy', 'promote', 'push', 'bear_on', 'push_button', 'drive', 'advertize', 'advertise', 'agitate', 'button', 'get-up-and-go', 'thrust', 'fight', 'tug', 'labour', 'crusade', 'campaign', 'labor', 'force'}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "word = \"push\"\n",
    "synonyms = get_synonyms(word)\n",
    "print(f\"Synonyms of {word}: {synonyms}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid = True\n",
    "try:\n",
    "    number = int('100 in')\n",
    "except ValueError:\n",
    "    valid = False\n",
    "\n",
    "valid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
