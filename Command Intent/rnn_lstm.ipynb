{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial 1 : RNN (LSTM) \n",
    "can handle sequential data (sentences) effectively\n",
    "1. Embedding layer\n",
    "2. LSTM layer (sequence processing)\n",
    "3. Two dense layers for classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yazmi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\yazmi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yazmi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yazmi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "import utils\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique intents: 14\n",
      "Number of responses: 14\n",
      "Number of examples: 340\n",
      "Number of examples: 340\n",
      "Unique intents: ['Varibale Declaration', 'Function Declaration', 'Class Declaration', 'Assignment Operation', 'Conditional Statement', 'Iterative Statement', 'Array Operation', 'Bitwise Operation', 'Mathematical Operation', 'Membership Operation', 'Casting', 'I/O Operation', 'Assertion', 'Libraries']\n",
      "Responses: ['Variable declaration intent detected', 'function declaration intent detected', 'class declaration intent detected', 'assignment intent detected', 'conditional statement intent detected', 'iterative statement intent detected', 'array operation intent detected', 'bitwise operation intent detected', 'mathematical operation intent detected', 'membership operation intent detected', 'casting intent detected', 'I/O operation intent detected', 'assertion intent detected', 'library intent detected']\n",
      "Samples: ['temp equals string', 'y equals', 'x equals', 'is finished equals false', 'is verified equals true']\n"
     ]
    }
   ],
   "source": [
    "dataset_path = './final_intents.json'\n",
    "unique_intents, corpus, corpus_intents, responses = utils.load_data(dataset_path)\n",
    "\n",
    "# print shapes and sizes of the dataset\n",
    "print('Number of unique intents:', len(unique_intents))\n",
    "print('Number of responses:', len(responses))\n",
    "print('Number of examples:', len(corpus))\n",
    "print('Number of examples:', len(corpus_intents))\n",
    "\n",
    "# print samples of the dataset\n",
    "print('Unique intents:', unique_intents)\n",
    "print('Responses:', responses)\n",
    "print('Samples:', corpus[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Tokenizing and Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'<unk>': 1, 'a': 2, 'the': 3, 'to': 4, 'and': 5, 'is': 6, 'variable': 7, 'x': 8, 'named': 9, 'that': 10, 'name': 11, 'of': 12, 'y': 13, 'list': 14, 'it': 15, 'if': 16, 'function': 17, 'in': 18, 'create': 19, 'takes': 20, 'class': 21, 'assign': 22, 'define': 23, 'value': 24, 'new': 25, 'with': 26, 'set': 27, 'declare': 28, 'from': 29, 'string': 30, 'user': 31, 'array': 32, 'numbers': 33, 'check': 34, 'an': 35, 'equal': 36, 'than': 37, 'for': 38, 'b': 39, 'equals': 40, 'age': 41, 'values': 42, 'write': 43, 'not': 44, 'float': 45, 'message': 46, 'get': 47, 'loop': 48, 'while': 49, 'library': 50, 'false': 51, 'true': 52, 'import': 53, 'parameters': 54, 'num': 55, 'by': 56, 'type': 57, 'number': 58, 'valid': 59, 'bitwise': 60, 'their': 61, 'temp': 62, 'initiate': 63, 'returns': 64, 'print': 65, 'salary': 66, 'divided': 67, 'int': 68, 'var': 69, 'arr': 70, 'called': 71, 'two': 72, 'sum': 73, 'names': 74, 'cast': 75, 'input': 76, 'count': 77, 'add': 78, 'person': 79, 'put': 80, 'less': 81, 'are': 82, 'use': 83, 'append': 84, 'operation': 85, 'on': 86, 'or': 87, 'power': 88, 'assert': 89, 'double': 90, 'as': 91, 'under': 92, 'search': 93, 'i': 94, 'range': 95, 'make': 96, 'convert': 97, 'empty': 98, 'username': 99, 'calculate': 100, 'display': 101, 'method': 102, 'length': 103, 'password': 104, 'show': 105, 'compute': 106, 'student': 107, 'extends': 108, 'inherits': 109, 'john': 110, 'update': 111, 'greater': 112, 'whether': 113, 'nums': 114, 'current': 115, 'perform': 116, 'shift': 117, 'tuple': 118, 'change': 119, 'result': 120, 'finished': 121, 'item': 122, 'height': 123, 'active': 124, 'open': 125, 'fruits': 126, 'apple': 127, 'banana': 128, 'dictionary': 129, 'subtract': 130, 'arguments': 131, 'parameter': 132, 'max': 133, 'c': 134, 'z': 135, 'email': 136, 'car': 137, 'flag': 138, 'more': 139, 'iterate': 140, 'using': 141, 'times': 142, 'find': 143, 'clear': 144, 'nodes': 145, 'xor': 146, 'plus': 147, 'words': 148, 'take': 149, 'read': 150, 'ask': 151, 'enter': 152, 'output': 153, 'include': 154, 'verified': 155, 'integer': 156, 'score': 157, 'pi': 158, 'distance': 159, 'hello': 160, 'bool': 161, 'items': 162, 'initialize': 163, 'cherry': 164, 'test': 165, 'has': 166, 'multiply': 167, 'first': 168, 'key': 169, 'str': 170, 'average': 171, 'details': 172, 'log': 173, 'employee': 174, 'customer': 175, 'book': 176, 'vehicle': 177, 'super': 178, 'base': 179, 'noah': 180, 'store': 181, 'size': 182, 'smaller': 183, 'same': 184, 'until': 185, 'iterates': 186, 'all': 187, 'elements': 188, 'sort': 189, 'remove': 190, 'reverse': 191, 'min': 192, 'left': 193, 'right': 194, 'd': 195, 'remainder': 196, 'val': 197, 'minus': 198, 'node': 199, 'visited': 200, 'word': 201, 'error': 202, 'following': 203, 'program': 204, 'prompt': 205, 'feedback': 206, 'warning': 207, 'ensure': 208, 'confirm': 209, 'math': 210, 'os': 211, 'json': 212, 'numpy': 213, 'cv': 214, 'quantity': 215, 'weight': 216, 'temperature': 217, 'velocity': 218, 'precision': 219, 'ratio': 220, 'admin': 221, 'greeting': 222, 'world': 223, 'title': 224, 'introduction': 225, 'programming': 226, 'note': 227, 'remember': 228, 'save': 229, 'boolean': 230, 'temperatures': 231, 'inventory': 232, 'config': 233, 'debug': 234, 'verbose': 235, 'func': 236, 'greet': 237, 'argument': 238, 'integers': 239, 'strings': 240, 'me': 241, 'call': 242, 'sequence': 243, 'balance': 244, 'account': 245, 'depth': 246, 'breadth': 247, 'binary': 248, 'functions': 249, 'target': 250, 'cosine': 251, 'similarity': 252, 'euclidean': 253, 'best': 254, 'concatenate': 255, 'login': 256, 'authenticate': 257, 'level': 258, 'area': 259, 'width': 260, 'which': 261, 'sub': 262, 'derived': 263, 'parent': 264, 'consumer': 265, 'my': 266, 'insert': 267, 'into': 268, 'am': 269, 'statement': 270, 'copy': 271, 'character': 272, 'adam': 273, 'none': 274, 'runs': 275, 'over': 276, 'n': 277, 'j': 278, 'condition': 279, 'maximum': 280, 'minimum': 281, 'extend': 282, 'cuurent': 283, 'ages': 284, 'delete': 285, 'vars': 286, 'vals': 287, 'do': 288, 'have': 289, 'divide': 290, 'modulus': 291, 'raise': 292, 'square': 293, 'root': 294, 'multiplied': 295, 'mod': 296, 'wight': 297, 'unique': 298, 't': 299, 'promt': 300, 'err': 301, 'ends': 302, 'here': 303, 'address': 304, 'phone': 305, 's': 306, 'date': 307, 'birth': 308, 'year': 309, 'request': 310, 'provide': 311, 'favorite': 312, 'color': 313, 'zip': 314, 'code': 315, 'success': 316, 'final': 317, 'total': 318, 'welcome': 319, 'null': 320, 'zero': 321, 'start': 322, 'end': 323, 'expected': 324, 'random': 325, 'sys': 326, 'work': 327, 'data': 328, 'datetime': 329, 'pandas': 330, 'matplotlib': 331, 'pytorch': 332, 'imports': 333, 'tensor': 334, 'flow': 335, 'seaborn': 336, 'nltk': 337, 'scikit': 338, 'learn': 339, 'csv': 340}\n",
      "Vocabulary Size: 341\n",
      "Shape of Input Sequence (# of examples, longest sequence length): (340, 20)\n",
      "Sample Input Sequence: [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 62 40 30]\n"
     ]
    }
   ],
   "source": [
    "# create a tokenizer\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(filters='', oov_token='<unk>')\n",
    "\n",
    "# fit the tokenizer on the corpus -> updates internal vocabulary based on corpus\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "# convert the corpus to sequences of integers -> each word is replaced by its index in the vocabulary for each sentence\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "# pad the sequences to the same length -> add padding tokens to the beginning of each sequence to fit the longest sequence\n",
    "padded_sequences = keras.preprocessing.sequence.pad_sequences(sequences, padding='pre')\n",
    "\n",
    "# get the number of unique words (vocabulary size)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# print the tokenizer properties\n",
    "print('Vocabulary:', tokenizer.word_index)\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Shape of Input Sequence (# of examples, longest sequence length):', padded_sequences.shape)\n",
    "print('Sample Input Sequence:', padded_sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Feature Extraction\n",
    "map intents to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Varibale Declaration', 1: 'Function Declaration', 2: 'Class Declaration', 3: 'Assignment Operation', 4: 'Conditional Statement', 5: 'Iterative Statement', 6: 'Array Operation', 7: 'Bitwise Operation', 8: 'Mathematical Operation', 9: 'Membership Operation', 10: 'Casting', 11: 'I/O Operation', 12: 'Assertion', 13: 'Libraries'}\n",
      "Categorial vector shape: (340, 14)\n"
     ]
    }
   ],
   "source": [
    "# dictionary that maps each intent to a unique index\n",
    "intent_to_index = {intent: index for index, intent in enumerate(unique_intents)}\n",
    "\n",
    "# list for each sentence mapped to its corresponding intent index \n",
    "corpus_intent_mapped_to_index = [intent_to_index[intent] for intent in corpus_intents]\n",
    "\n",
    "# the number of classes to classify a sentence into\n",
    "number_of_classes = len(intent_to_index)\n",
    "\n",
    "# convert intent_to_index to index_to_intent \n",
    "index_to_intent = {index: intent for intent, index in intent_to_index.items()} \n",
    "\n",
    "print(index_to_intent)\n",
    "\n",
    "# one hot encoding for the intents -> length of each vector is equal to the number of classes\n",
    "# each sequence in the dataset is represented as a one-hot encoded vector that represents the intent of the sequence\n",
    "one_hot_encoded_intents = keras.utils.to_categorical(corpus_intent_mapped_to_index, number_of_classes)\n",
    "\n",
    "print('Categorial vector shape:', one_hot_encoded_intents.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Dimension: 14, Output Dimension: 14\n",
      "WARNING:tensorflow:From c:\\Users\\yazmi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 128)         43648     \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 128)               98816     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 14)                910       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 151630 (592.30 KB)\n",
      "Trainable params: 151630 (592.30 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# the input is the the padded sequences with the target value being the one-hot encoded intents\n",
    "input_dimenstion = len(unique_intents)\n",
    "\n",
    "# the output is the one-hot encoded intents\n",
    "output_dimenstion = one_hot_encoded_intents.shape[1]\n",
    "\n",
    "print(f\"Input Dimension: {input_dimenstion}, Output Dimension: {output_dimenstion}\")\n",
    "\n",
    "# Model description\n",
    "# The model is a sequential model that consists of:\n",
    "# 1. An embedding layer that converts the input sequences to dense vectors of fixed size\n",
    "# 2. A Bidirectional LSTM layer that processes the input sequences in both directions\n",
    "# 3. A Dense layer with 64 units and ReLU activation function\n",
    "# 4. A Dropout layer with a dropout rate of 0.5\n",
    "# 5. A Dense layer with the output dimension and softmax activation function for multi-class classification\n",
    "\n",
    "# define parameters\n",
    "epochs = 100\n",
    "\n",
    "# the embedding dimension is the size of the vector for which each word is represented\n",
    "# the embedding layer of a neural network, output_dim refers to the size of the dense vectors that the layer will generate for each input token (word). \n",
    "# essentially, it is the number of dimensions in which each word will be represented.\n",
    "embedding_dimension = 128\n",
    "\n",
    "# lstm units\n",
    "lstm_units = 64\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dimension),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(lstm_units, dropout=0.2)),\n",
    "    keras.layers.Dense(lstm_units, activation='relu'),\n",
    "    keras.layers.Dropout(0.4),\n",
    "    keras.layers.Dense(output_dimenstion, activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yazmi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yazmi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yazmi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yazmi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x29a2893b670>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_sequences, one_hot_encoded_intents, epochs=epochs, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 915ms/step\n",
      "Predicted Intent: Varibale Declaration\n"
     ]
    }
   ],
   "source": [
    "user_input = input(\"Sentence:\")\n",
    "\n",
    "# actual_intent = input(\"Intent:\")\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences([user_input])\n",
    "\n",
    "test_padded_sequences = keras.preprocessing.sequence.pad_sequences(test_sequences, padding='pre')\n",
    "\n",
    "predictions = model.predict(test_padded_sequences)\n",
    "\n",
    "predicted_intent_index = np.argmax(predictions)\n",
    "\n",
    "predicted_intent = index_to_intent[predicted_intent_index]\n",
    "\n",
    "print(f\"Predicted Intent: {predicted_intent}\")\n",
    "\n",
    "# loss, accuracy = model.evaluate(test_padded_sequences, np.array([actual_intent]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
