{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from TorchCRF import CRF\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.2+cu118'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Sentence #', 'Word', 'Tag', 'Intent'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def read_dataset():\n",
    "    data = pd.read_csv('./ner_dataset/ner_dataset.csv', encoding='latin1')\n",
    "\n",
    "    # remove white spaces from column names\n",
    "    data.columns = data.columns.str.strip()\n",
    "\n",
    "    print(data.columns)\n",
    "    # print(data.columns)\n",
    "    # Group by 'Sentence #' and aggregate\n",
    "    grouped_data = data.groupby('Sentence #').agg({\n",
    "        'Word': lambda x: ' '.join(x),  # Join words into a single sentence\n",
    "        'Tag': lambda x: list(x),       # Collect tags into a list\n",
    "        'Intent': lambda x: x     # Collect intents into a list\n",
    "    }).reset_index()  # Reset index to make 'Sentence #' a regular column\n",
    "\n",
    "    return data, grouped_data\n",
    "\n",
    "\n",
    "def prepare_data(dataframe):\n",
    "    dataset = []\n",
    "    for _, row in dataframe.iterrows():\n",
    "        sentence = row['Word']\n",
    "        tags = row['Tag']\n",
    "        intents = row['Intent'][0]\n",
    "        dataset.append((sentence, tags, intents))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "data, grouped_data = read_dataset()\n",
    "\n",
    "prepared_dataset = prepare_data(grouped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the dataset: 304\n",
      "Number of unique tags in the dataset: 6\n",
      "Number of unique intents in the dataset: 1\n",
      "Unique tags in the dataset: [' B-VAR' ' I-VAR' ' O' ' B-VAL' ' I-VAL' ' B-TYPE']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique words in the dataset:\", len(data['Word'].unique()) )# number of unique words in the dataset\n",
    "print(\"Number of unique tags in the dataset:\", len(data['Tag'].unique())) # number of unique tags in the dataset\n",
    "print(\"Number of unique intents in the dataset:\", len(data['Intent'].unique())) #number of unique intents in the dataset\n",
    "print(\"Unique tags in the dataset:\", data['Tag'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting unique words and labels from data\n",
    "words = list(data['Word'].unique())\n",
    "tags = list(data['Tag'].unique())\n",
    "\n",
    "# word is key and its value is corresponding index\n",
    "word_to_index = {word.strip() : i + 2 for i, word in enumerate(words)}\n",
    "word_to_index[\"UNK\"] = 1\n",
    "word_to_index[\"PAD\"] = 0\n",
    "\n",
    "# label is key and value is index.\n",
    "tag_to_index = {tag.strip() : i + 1 for i, tag in enumerate(tags)}\n",
    "tag_to_index[\"PAD\"] = 0\n",
    "\n",
    "index_to_word = {i: word for word, i in word_to_index.items()}\n",
    "index_to_tag = {i: tag for tag, i in tag_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the mappings to file\n",
    "import json\n",
    "with open('word_to_index.json', 'w') as f:\n",
    "    json.dump(word_to_index, f, indent=4)\n",
    "\n",
    "with open('tag_to_index.json', 'w') as f:\n",
    "    json.dump(tag_to_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_to_index.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    _word2idx = data[\"word2idx\"]\n",
    "    words_list = [word for word in _word2idx.keys()]\n",
    "    data[\"idx2word\"] = words_list\n",
    "\n",
    "with open('word_to_index.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tag_to_index.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    _tag2idx = data[\"word2idx\"]\n",
    "    tag_list = [word for word in _tag2idx.keys()]\n",
    "    data[\"idx2word\"] = tag_list\n",
    "\n",
    "with open('tag_to_index.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples sample: (' is approved equals clustering algorithms', ['B-VAR', 'I-VAR', 'O', 'B-VAL', 'I-VAL'], 'variable_declaration')\n"
     ]
    }
   ],
   "source": [
    "# This is a class te get sentence. The each sentence will be list of tuples with its tag and pos.\n",
    "class Sentence(object):\n",
    "    def __init__(self, df):\n",
    "        self.n_sent = 0\n",
    "        self.df = df\n",
    "        self.empty = False\n",
    "        agg = lambda s : [(word.strip(), tag.strip(), intent.strip()) for word, tag, intent in zip(s['Word'].values.tolist(),\n",
    "                                                       s['Tag'].values.tolist(),\n",
    "                                                       s['Intent'].values.tolist())]\n",
    "        self.grouped = self.df.groupby(\"Sentence #\").apply(agg)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "        \n",
    "    def get_text(self):\n",
    "        try:\n",
    "            s = self.grouped[self.n_sent]\n",
    "            self.n_sent +=1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def records_to_tuples(self):\n",
    "        dataset = []\n",
    "\n",
    "        grouped_data = data.groupby('Sentence #').agg({\n",
    "        'Word': lambda x: ''.join(x),  # Join words into a single sentence\n",
    "        'Tag': lambda x: list(x.str.strip()),       # Collect tags into a list\n",
    "        'Intent': lambda x: x.str.strip()     # Collect intents into a list\n",
    "        }).reset_index()\n",
    "\n",
    "        for _, row in grouped_data.iterrows():\n",
    "            sentence = row['Word']\n",
    "            tags = row['Tag']\n",
    "            intents = row['Intent'][0]\n",
    "            dataset.append((sentence, tags, intents))\n",
    "\n",
    "        return dataset\n",
    "        \n",
    "#Displaying one full sentence\n",
    "getter = Sentence(data)\n",
    "\n",
    "examples = getter.records_to_tuples()\n",
    "\n",
    "print(\"Examples sample:\", examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset.txt', 'w') as f:\n",
    "    for example in examples:\n",
    "        f.write(str(example[0].split()) + '\\t' + str(example[1]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 306\n",
      "Tag to index: {'B-VAR': 1, 'I-VAR': 2, 'O': 3, 'B-VAL': 4, 'I-VAL': 5, 'B-TYPE': 6, 'PAD': 0}\n",
      "Index to tag: {0: 'PAD', 1: 'B-VAR', 2: 'I-VAR', 3: 'O', 4: 'B-VAL', 5: 'I-VAL', 6: 'B-TYPE'}\n",
      "Max length of sentence: 20\n",
      "Sample sentence: is approved equals clustering algorithms\n",
      "Sample tags: ['B-VAR', 'I-VAR', 'O', 'B-VAL', 'I-VAL']\n"
     ]
    }
   ],
   "source": [
    "# We should convert each sentence to integers\n",
    "#Getting unique words and labels from data -> our vocab\n",
    "words = list(data['Word'].unique())\n",
    "tags = list(data['Tag'].unique())\n",
    "\n",
    "# 1. Each word to integer\n",
    "# word is key and its value is corresponding index\n",
    "word_to_index = {word.strip() : i + 2 for i, word in enumerate(words)}\n",
    "word_to_index['UNK'] = 1\n",
    "word_to_index['PAD'] = 0\n",
    "\n",
    "vocab_size = len(word_to_index)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "# 2. Each label to integer\n",
    "# label is key and value is index.\n",
    "tag_to_index = {tag.strip() : i + 1 for i, tag in enumerate(tags)}\n",
    "tag_to_index['PAD'] = 0\n",
    "\n",
    "print(\"Tag to index:\", tag_to_index)\n",
    "\n",
    "# conver _ to index to index_ to word\n",
    "index_to_word = dict(sorted({i: word for word, i in word_to_index.items()}.items(), key=lambda item: item[0]))\n",
    "index_to_tag = dict(sorted({i: tag for tag, i in tag_to_index.items()}.items(), key=lambda item: item[0]))\n",
    "\n",
    "print(\"Index to tag:\", index_to_tag)\n",
    "\n",
    "sentences = [example[0][1:] for example in examples]\n",
    "\n",
    "max_len = max(len(s.split()) for s in sentences)\n",
    "\n",
    "print(\"Max length of sentence:\", max_len)\n",
    "\n",
    "tags = [example[1] for example in examples]\n",
    "\n",
    "print(\"Sample sentence:\", sentences[0])\n",
    "\n",
    "print(\"Sample tags:\", tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m line[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m [word\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m line[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m     23\u001b[0m line[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<END>\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m line[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, \u001b[43mline\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(line[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     27\u001b[0m line[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m [tag\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m line[\u001b[38;5;241m1\u001b[39m]]\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Padding function\n",
    "# def pad_sequences(sequences, max_len, pad_value=0):\n",
    "#     padded_sequences = []\n",
    "#     for seq in sequences:\n",
    "#         if len(seq) < max_len:\n",
    "#             padded_seq = seq + [pad_value] * (max_len - len(seq))\n",
    "#         else:\n",
    "#             padded_seq = seq[:max_len]\n",
    "#         padded_sequences.append(padded_seq)\n",
    "#     return padded_sequences\n",
    "test = \"['is', '2', 'a', 'variable']\"\n",
    "test_list = list(map(str, test[1:-1].split(',')))\n",
    "\n",
    "test_list = [word.strip().replace('\\'', '') for word in test_list]\n",
    "test_list\n",
    "sentence = [\"<START>\"]\n",
    "tago = [\"<START>\"]\n",
    "line = \"['is', 'approved', 'equals', 'clustering', 'algorithms']  ['B-VAR', 'I-VAR', 'O', 'B-VAL', 'I-VAL']\"\n",
    "\n",
    "line = line.split('\\t')\n",
    "line[0] = list(map(str, line[0][1:-1].split(',')))\n",
    "line[0] = [word.strip().replace('\\'', '') for word in line[0]]\n",
    "line[0].append('<END>')\n",
    "\n",
    "line[1] = list(map(str, line[1][1:-1].split(',')))\n",
    "print(line[1])\n",
    "line[1] = [tag.strip().replace('\\'', '') for tag in line[1]]\n",
    "line[1].append('<END>')\n",
    "\n",
    "print(line[0])\n",
    "print(line[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "new(): invalid data type 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 36\u001b[0m\n\u001b[0;32m     27\u001b[0m train_sentences, val_sentences, train_tags, val_tags \u001b[38;5;241m=\u001b[39m train_test_split(sentences, tags, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# # Create Dataset and DataLoader for training and validation\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# train_dataset = NERDataset(train_sentences, train_tags, word_to_index, tag_to_index)\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# val_dataset = NERDataset(val_sentences, val_tags, word_to_index, tag_to_index)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mNERDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_tags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m NERDataset(val_sentences, val_tags)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollate_fn\u001b[39m(batch):\n",
      "Cell \u001b[1;32mIn[24], line 16\u001b[0m, in \u001b[0;36mNERDataset.__init__\u001b[1;34m(self, sentences, tags)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentences, tags):\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentences \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mtensor(sentence, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences]\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mtensor(tag, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong) \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m tags]\n",
      "Cell \u001b[1;32mIn[24], line 16\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentences, tags):\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentences \u001b[38;5;241m=\u001b[39m [\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences]\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mtensor(tag, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong) \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m tags]\n",
      "\u001b[1;31mTypeError\u001b[0m: new(): invalid data type 'str'"
     ]
    }
   ],
   "source": [
    "# Dataset class\n",
    "# class NERDataset(Dataset):\n",
    "#     def __init__(self, sentences, tags, word_to_index, tag_to_index):\n",
    "#         self.sentences = [[word_to_index[word] for word in sentence] for sentence in sentences]\n",
    "#         self.tags = [[tag_to_index[tag] for tag in tag_seq] for tag_seq in tags]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.sentences)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return torch.tensor(self.sentences[idx], dtype=torch.long), torch.tensor(self.tags[idx], dtype=torch.long)\n",
    "\n",
    "# Dataset class\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, tags):\n",
    "        self.sentences = [torch.tensor(sentence, dtype=torch.long) for sentence in sentences]\n",
    "        self.tags = [torch.tensor(tag, dtype=torch.long) for tag in tags]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx], self.tags[idx]\n",
    "\n",
    "\n",
    "# Split the data\n",
    "train_sentences, val_sentences, train_tags, val_tags = train_test_split(sentences, tags, test_size=0.1, random_state=42)\n",
    "\n",
    "# # Create Dataset and DataLoader for training and validation\n",
    "# train_dataset = NERDataset(train_sentences, train_tags, word_to_index, tag_to_index)\n",
    "# val_dataset = NERDataset(val_sentences, val_tags, word_to_index, tag_to_index)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "train_dataset = NERDataset(train_sentences, train_tags)\n",
    "val_dataset = NERDataset(val_sentences, val_tags)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sentences, tags = zip(*batch)\n",
    "    sentences_padded = pad_sequence(sentences, batch_first=True, padding_value=word_to_index['PAD'])\n",
    "    tags_padded = pad_sequence(tags, batch_first=True, padding_value=tag_to_index['PAD'])\n",
    "    return sentences_padded, tags_padded\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Model Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMCRF(nn.Module):\n",
    "    def __init__(self, vocab_size, number_of_tags, embedding_dim=128, hidden_dim=128):\n",
    "        super(BiLSTMCRF, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=1, bidirectional=True)\n",
    "        self.hidden_to_tag = nn.Linear(hidden_dim, number_of_tags)\n",
    "        self.crf = CRF(number_of_tags, batch_first=True)\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        embeddings = self.embedding(sentences)\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "        emissions = self.hidden_to_tag(lstm_out)\n",
    "        return emissions\n",
    "\n",
    "    def loss(self, emissions, tags, mask):\n",
    "        return -self.crf(emissions, tags, mask=mask, reduction='mean')\n",
    "\n",
    "    def predict(self, sentences, mask):\n",
    "        emissions = self.forward(sentences)\n",
    "        return self.crf.decode(emissions, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model parameters\n",
    "vocab_size = len(word_to_index)\n",
    "number_of_tags = len(tag_to_index)\n",
    "embedding_dimension = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yazmi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\yazmi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTMCRF(vocab_size=vocab_size, number_of_tags=number_of_tags)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "the first two dimensions of emissions and tags must match, got (1, 11) and (1, 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m      8\u001b[0m emissions \u001b[38;5;241m=\u001b[39m model(sentences)\n\u001b[1;32m----> 9\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43memissions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[10], line 16\u001b[0m, in \u001b[0;36mBiLSTMCRF.loss\u001b[1;34m(self, emissions, tags, mask)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, emissions, tags, mask):\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrf\u001b[49m\u001b[43m(\u001b[49m\u001b[43memissions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yazmi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yazmi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yazmi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\TorchCRF\\__init__.py:90\u001b[0m, in \u001b[0;36mCRF.forward\u001b[1;34m(self, emissions, tags, mask, reduction)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     65\u001b[0m         emissions: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     68\u001b[0m         reduction: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     69\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m     70\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the conditional log likelihood of a sequence of tags given emission scores.\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;124;03m        reduction is ``none``, ``()`` otherwise.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 90\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43memissions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m reduction \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_mean\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minvalid reduction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreduction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\yazmi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\TorchCRF\\__init__.py:155\u001b[0m, in \u001b[0;36mCRF._validate\u001b[1;34m(self, emissions, tags, mask)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tags \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m emissions\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m!=\u001b[39m tags\u001b[38;5;241m.\u001b[39mshape:\n\u001b[1;32m--> 155\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    156\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe first two dimensions of emissions and tags must match, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    157\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(emissions\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(tags\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m emissions\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m!=\u001b[39m mask\u001b[38;5;241m.\u001b[39mshape:\n",
      "\u001b[1;31mValueError\u001b[0m: the first two dimensions of emissions and tags must match, got (1, 11) and (1, 12)"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for sentences, tags in train_loader:\n",
    "        mask = sentences != 0  # Mask for padding\n",
    "        optimizer.zero_grad()\n",
    "        emissions = model(sentences)\n",
    "        loss = model.loss(emissions, tags, mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for sentences, tags in val_loader:\n",
    "            mask = sentences != 0\n",
    "            emissions = model(sentences)\n",
    "            loss = model.loss(emissions, tags, mask)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Training Loss: {total_train_loss / len(train_loader)}, Validation Loss: {total_val_loss / len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for sentences, tags in val_loader:\n",
    "        mask = sentences != 0  # Mask for padding\n",
    "        predictions = model.predict(sentences, mask)\n",
    "        for sentence, prediction, tag in zip(sentences, predictions, tags):\n",
    "            sentence = sentence.tolist()\n",
    "            prediction = [index_to_tag[p] for p in prediction]\n",
    "            tag = [index_to_tag[t] for t in tag.tolist()]\n",
    "            print(f\"Sentence: {sentence}\")\n",
    "            print(f\"Prediction: {prediction}\")\n",
    "            print(f\"Ground Truth: {tag}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
