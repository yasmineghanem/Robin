{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **NER Custom Model**\n",
    "#### Bi-LSTM with CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import torch.optim as optim\n",
    "from TorchCRF import CRF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Sentence #', 'Word', 'Tag', 'Intent'], dtype='object')\n",
      "20\n",
      "(' is  approved  equals  clustering  algorithms', [' B-VAR', ' I-VAR', ' O', ' B-VAL', ' I-VAL'], ' variable_declaration')\n"
     ]
    }
   ],
   "source": [
    "def read_dataset():\n",
    "    data = pd.read_csv('./ner_dataset/ner_dataset.csv', encoding='latin1')\n",
    "\n",
    "    # remove white spaces from column names\n",
    "    data.columns = data.columns.str.strip()\n",
    "\n",
    "    print(data.columns)\n",
    "    # print(data.columns)\n",
    "    # Group by 'Sentence #' and aggregate\n",
    "    grouped_data = data.groupby('Sentence #').agg({\n",
    "        'Word': lambda x: ' '.join(x),  # Join words into a single sentence\n",
    "        'Tag': lambda x: list(x),       # Collect tags into a list\n",
    "        'Intent': lambda x: x     # Collect intents into a list\n",
    "    }).reset_index()  # Reset index to make 'Sentence #' a regular column\n",
    "\n",
    "    # Display the grouped and aggregated data\n",
    "    # print(grouped_data.columns)\n",
    "    return data, grouped_data\n",
    "\n",
    "def prepare_data(dataframe):\n",
    "    dataset = []\n",
    "    for index, row in dataframe.iterrows():\n",
    "        sentence = row['Word']\n",
    "        tags = row['Tag']\n",
    "        intents = row['Intent'][0]\n",
    "        dataset.append((sentence, tags, intents))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "data, grouped_data = read_dataset()\n",
    "\n",
    "prepared_dataset = prepare_data(grouped_data)\n",
    "\n",
    "# get maximum length of sentence\n",
    "max_sentence_length = max([len(sentence.split()) for sentence, _, _ in prepared_dataset])\n",
    "\n",
    "print(max_sentence_length)\n",
    "\n",
    "print(prepared_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(prepared_dataset, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1655\n",
      "Vocabulary size: 305, Tag set size: 7, Intent set size: 1\n"
     ]
    }
   ],
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, data, word_to_ix, tag_to_ix, intent_to_ix):\n",
    "        self.data = data\n",
    "        self.word_to_ix = word_to_ix\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.intent_to_ix = intent_to_ix\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence, tags, intent = self.data[idx]\n",
    "        words = sentence.split()\n",
    "        word_indices = [self.word_to_ix[w] for w in words]\n",
    "        tag_indices = [self.tag_to_ix[t] for t in tags]\n",
    "        intent_index = self.intent_to_ix[intent]\n",
    "        return torch.tensor(word_indices, dtype=torch.long), torch.tensor(tag_indices, dtype=torch.long), torch.tensor(intent_index, dtype=torch.long)\n",
    "\n",
    "print(len(data['Word'].values))\n",
    "\n",
    "# Create vocabulary and tag dictionaries\n",
    "words = set(data['Word'].values)\n",
    "tags = set(data['Tag'].values)\n",
    "intents = set(data['Intent'].values)\n",
    "\n",
    "# print(words)\n",
    "\n",
    "word_to_ix = {word.strip(): i for i, word in enumerate(words)}\n",
    "tag_to_ix = {tag: i for i, tag in enumerate(tags)}\n",
    "intent_to_ix = {intent: i for i, intent in enumerate(intents)}\n",
    "\n",
    "# Add a special token for OOV words and padding\n",
    "word_to_ix['<OOV>'] = len(word_to_ix)\n",
    "word_to_ix['<PAD>'] = len(word_to_ix)\n",
    "tag_to_ix['<PAD>'] = len(tag_to_ix)\n",
    "\n",
    "word_vocab_size = len(word_to_ix)\n",
    "tagset_size = len(tag_to_ix)\n",
    "intent_vocab_size = len(intent_to_ix)\n",
    "\n",
    "print(f'Vocabulary size: {word_vocab_size}, Tag set size: {tagset_size}, Intent set size: {intent_vocab_size}')\n",
    "\n",
    "# print(word_to_ix['with'])\n",
    "\n",
    "# print(word_to_ix)\n",
    "# print(tag_to_ix)\n",
    "\n",
    "# print(len(word_to_ix))\n",
    "# print(len(tag_to_ix))\n",
    "# print(len(intent_to_ix))\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = NERDataset(train_data, word_to_ix, tag_to_ix, intent_to_ix)\n",
    "val_dataset = NERDataset(val_data, word_to_ix, tag_to_ix, intent_to_ix)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    sentences, tags, intents = zip(*data)\n",
    "    max_len = max(len(s.split()) for s in sentences)\n",
    "    sentence_tensors = []\n",
    "    tag_tensors = []\n",
    "    for sentence, tag in zip(sentences, tags):\n",
    "        words = sentence.split()\n",
    "        word_indices = [word_to_ix.get(w, word_to_ix['<OOV>']) for w in words]\n",
    "        word_indices += [word_to_ix['<PAD>']] * (max_len - len(word_indices))\n",
    "        tag_indices = [tag_to_ix[t] for t in tag]\n",
    "        tag_indices += [tag_to_ix['<PAD>']] * (max_len - len(tag_indices))\n",
    "        sentence_tensors.append(torch.tensor(word_indices, dtype=torch.long))\n",
    "        tag_tensors.append(torch.tensor(tag_indices, dtype=torch.long))\n",
    "    intent_indices = [intent_to_ix[intent] for intent in intents]\n",
    "    sentence_tensors = torch.stack(sentence_tensors)\n",
    "    tag_tensors = torch.stack(tag_tensors)\n",
    "    intent_indices = torch.tensor(intent_indices, dtype=torch.long)\n",
    "    return sentence_tensors, tag_tensors, intent_indices\n",
    "\n",
    "train_sentences, train_tags, train_intents = prepare_data(train_data)\n",
    "val_sentences, val_tags, val_intents = prepare_data(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "word_vocab_size = len(word_to_ix)\n",
    "tagset_size = len(tag_to_ix)\n",
    "intent_vocab_size = len(intent_to_ix)\n",
    "\n",
    "word_embedding_dim = 100\n",
    "intent_embedding_dim = 50\n",
    "\n",
    "hidden_dim = 128\n",
    "lstm_input_dim = word_embedding_dim + intent_embedding_dim\n",
    "# word_embedding = nn.Embedding(word_vocab_size, word_embedding_dim)\n",
    "# intent_embedding = nn.Embedding(intent_vocab_size, intent_embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Layer\n",
    "\n",
    "# lstm = nn.LSTM(lstm_input_dim, hidden_dim, bidirectional=True, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden (Linear) layer and CRF\n",
    "# hidden2tag = nn.Linear(hidden_dim * 2, tagset_size)\n",
    "\n",
    "# crf = CRF(tagset_size, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERModel(nn.Module):\n",
    "    def __init__(self, word_vocab_size, word_embedding_dim, intent_vocab_size, intent_embedding_dim, lstm_input_dim, hidden_dim, tagset_size):\n",
    "        super(NERModel, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(word_vocab_size, word_embedding_dim)\n",
    "        self.intent_embedding = nn.Embedding(intent_vocab_size, intent_embedding_dim)\n",
    "        self.lstm = nn.LSTM(lstm_input_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim * 2, tagset_size)\n",
    "        self.crf = CRF(tagset_size)\n",
    "\n",
    "    def forward(self, sentence, intent):\n",
    "        word_embeds = self.word_embedding(sentence)\n",
    "        intent_embed = self.intent_embedding(intent).unsqueeze(1).repeat(1, word_embeds.size(1), 1)\n",
    "        lstm_input = torch.cat((word_embeds, intent_embed), dim=2)\n",
    "        lstm_out, _ = self.lstm(lstm_input)\n",
    "        emissions = self.hidden2tag(lstm_out)\n",
    "        emissions = emissions.transpose(0, 1)\n",
    "        return emissions\n",
    "\n",
    "    def loss(self, emissions, tags, mask):\n",
    "        tags = tags.transpose(0, 1)\n",
    "        mask = mask.transpose(0, 1)\n",
    "        return -self.crf(emissions, tags, mask=mask, reduction='mean')\n",
    "\n",
    "    def decode(self, emissions, mask):\n",
    "        emissions = emissions.transpose(0, 1)\n",
    "        mask = mask.transpose(0, 1)\n",
    "        return self.crf.decode(emissions, mask=mask)\n",
    "\n",
    "model = NERModel(word_vocab_size, word_embedding_dim, intent_vocab_size, intent_embedding_dim, lstm_input_dim, hidden_dim, tagset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 12\u001b[0m emissions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_intents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m mask \u001b[38;5;241m=\u001b[39m mask_padding(train_sentences, word_to_ix[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<PAD>\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mloss(emissions, train_tags, mask)\n",
      "File \u001b[1;32mc:\\Users\\yazmi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yazmi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[77], line 11\u001b[0m, in \u001b[0;36mNERModel.forward\u001b[1;34m(self, sentence, intent)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence, intent):\n\u001b[1;32m---> 11\u001b[0m     word_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     intent_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintent_embedding(intent)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, word_embeds\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     13\u001b[0m     lstm_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((word_embeds, intent_embed), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\yazmi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yazmi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yazmi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yazmi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:2237\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2231\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2232\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2233\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2234\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2235\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2236\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def mask_padding(tensor, padding_idx):\n",
    "    return tensor != padding_idx\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    emissions = model(train_sentences, train_intents)\n",
    "    mask = mask_padding(train_sentences, word_to_ix['<PAD>'])\n",
    "    loss = model.loss(emissions, train_tags, mask)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        emissions = model(val_sentences, val_intents)\n",
    "        mask = mask_padding(val_sentences, word_to_ix['<PAD>'])\n",
    "        predictions = model.decode(emissions, mask)\n",
    "        # Calculate evaluation metrics here using `predictions` and `val_tags`\n",
    "\n",
    "# The model is now trained, and you can perform evaluation as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding example\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in batchify(val_data, batch_size):\n",
    "        sentences, tags, intents = prepare_batch(batch)\n",
    "        emissions = model(sentences, intents)\n",
    "        mask = sentences != 0\n",
    "        predictions = model.decode(emissions, mask)\n",
    "        # Here, you can compare predictions with the true tags"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
