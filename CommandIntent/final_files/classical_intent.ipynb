{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from Logistic_Regression import *\n",
    "\n",
    "# Download stopwords\n",
    "# nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocessing function\n",
    "    # Convert all to lowercase\n",
    "    # Remove punctuations\n",
    "    # Remove stopwords\n",
    "\n",
    "stop_words = {'a', 'the', 'is', 'it', 'to', 'of', 'in', 'for', 'on', 'with', 'as', 'at', 'by', 'from',\n",
    "                 'an', 'be', 'are', 'was', 'were', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she',\n",
    "                   'we', 'they', 'me', 'him', 'her', 'us', 'them', 'my', 'your', 'his', 'its', 'our', 'their',\n",
    "                     'mine', 'yours', 'his', 'hers', 'ours', 'theirs', 'what', 'which', 'who', 'whom', 'whose',\n",
    "                       'where', 'when', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other',\n",
    "                         'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'too', 'very'}\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    text = ' '.join(word for word in text.split())\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict intent of custom input\n",
    "# def predict_intent(custom_sentence, model):\n",
    "#     processed_sentence = preprocess_text(custom_sentence)\n",
    "#     X_custom = vectorizer.transform([processed_sentence])\n",
    "#     predicted_intent = model.predict(X_custom)\n",
    "#     return predicted_intent[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data & Pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>sentence</th>\n",
       "      <th>processed_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Variable Declaration</td>\n",
       "      <td>make start time as double and initialize 0.000123</td>\n",
       "      <td>make start time as double and initialize 0000123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Variable Declaration</td>\n",
       "      <td>declare min value as integer and value 131313</td>\n",
       "      <td>declare min value as integer and value 131313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Variable Declaration</td>\n",
       "      <td>define settings as boolean and value false</td>\n",
       "      <td>define settings as boolean and value false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Variable Declaration</td>\n",
       "      <td>define y as integer and assign to 12345</td>\n",
       "      <td>define y as integer and assign to 12345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Variable Declaration</td>\n",
       "      <td>initialize k as string and initialize it with ...</td>\n",
       "      <td>initialize k as string and initialize it with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2865</th>\n",
       "      <td>Git Operation</td>\n",
       "      <td>push changes</td>\n",
       "      <td>push changes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2866</th>\n",
       "      <td>Git Operation</td>\n",
       "      <td>stage changes</td>\n",
       "      <td>stage changes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2867</th>\n",
       "      <td>Git Operation</td>\n",
       "      <td>push changes</td>\n",
       "      <td>push changes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2868</th>\n",
       "      <td>Git Operation</td>\n",
       "      <td>stage changes</td>\n",
       "      <td>stage changes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2869</th>\n",
       "      <td>Git Operation</td>\n",
       "      <td>discard changes</td>\n",
       "      <td>discard changes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2870 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    intent                                           sentence   \n",
       "0     Variable Declaration  make start time as double and initialize 0.000123  \\\n",
       "1     Variable Declaration      declare min value as integer and value 131313   \n",
       "2     Variable Declaration         define settings as boolean and value false   \n",
       "3     Variable Declaration            define y as integer and assign to 12345   \n",
       "4     Variable Declaration  initialize k as string and initialize it with ...   \n",
       "...                    ...                                                ...   \n",
       "2865         Git Operation                                       push changes   \n",
       "2866         Git Operation                                      stage changes   \n",
       "2867         Git Operation                                       push changes   \n",
       "2868         Git Operation                                      stage changes   \n",
       "2869         Git Operation                                    discard changes   \n",
       "\n",
       "                                     processed_sentence  \n",
       "0      make start time as double and initialize 0000123  \n",
       "1         declare min value as integer and value 131313  \n",
       "2            define settings as boolean and value false  \n",
       "3               define y as integer and assign to 12345  \n",
       "4     initialize k as string and initialize it with ...  \n",
       "...                                                 ...  \n",
       "2865                                       push changes  \n",
       "2866                                      stage changes  \n",
       "2867                                       push changes  \n",
       "2868                                      stage changes  \n",
       "2869                                    discard changes  \n",
       "\n",
       "[2870 rows x 3 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load JSON file\n",
    "with open('../intent_detection_dataset/final_intents_dataset.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Convert to DataFrame\n",
    "rows = []\n",
    "for intent, sentences in data.items():\n",
    "    for sentence in sentences:\n",
    "        rows.append({'intent': intent, 'sentence': sentence})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Apply preprocessing\n",
    "df['processed_sentence'] = df['sentence'].apply(preprocess_text)\n",
    "\n",
    "#Display the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### TF-IDF ###########\n",
    "\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# X = vectorizer.fit_transform(df['processed_sentence'])\n",
    "# y = df['intent']\n",
    "\n",
    "########### N-Gram ###########\n",
    "# vectorizer = CountVectorizer(ngram_range=(1, 2))  # Using unigrams and bigrams\n",
    "\n",
    "# vectorizer = TfidfVectorizer(ngram_range=(1, 2))  # Using unigrams and bigrams\n",
    "\n",
    "# X = vectorizer.fit_transform(df['processed_sentence'])\n",
    "# y = df['intent']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2870,)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "\n",
    "# Extract N-grams\n",
    "def extract_ngrams(text, n=3):\n",
    "    words = text.split()\n",
    "    ngrams = []\n",
    "    for i in range(1, n + 1):\n",
    "        ngrams += [' '.join(words[j:j+i]) for j in range(len(words) - i + 1)]\n",
    "    return ngrams\n",
    "\n",
    "# Compute term frequencies\n",
    "def compute_term_frequencies(corpus):\n",
    "    term_frequencies = defaultdict(Counter)\n",
    "    for idx, document in enumerate(corpus):\n",
    "        ngrams = extract_ngrams(document)\n",
    "        term_frequencies[idx] = Counter(ngrams)\n",
    "    return term_frequencies\n",
    "\n",
    "# Compute document frequencies\n",
    "def compute_document_frequencies(term_frequencies):\n",
    "    document_frequencies = Counter()\n",
    "    for tf in term_frequencies.values():\n",
    "        for term in tf.keys():\n",
    "            document_frequencies[term] += 1\n",
    "    return document_frequencies\n",
    "\n",
    "# Compute TF-IDF\n",
    "def compute_tfidf(corpus):\n",
    "    term_frequencies = compute_term_frequencies(corpus)\n",
    "    document_frequencies = compute_document_frequencies(term_frequencies)\n",
    "    N = len(corpus)\n",
    "    tfidf = defaultdict(dict)\n",
    "    for idx, tf in term_frequencies.items():\n",
    "        for term, count in tf.items():\n",
    "            tfidf[idx][term] = (count / len(tf)) * math.log(N / (document_frequencies[term] + 1))\n",
    "    return tfidf, document_frequencies\n",
    "\n",
    "# Create TF-IDF matrix\n",
    "def create_tfidf_matrix(tfidf, vocabulary):\n",
    "    tfidf_matrix = np.zeros((len(tfidf), len(vocabulary)))\n",
    "    for doc_idx, term_scores in tfidf.items():\n",
    "        for term, score in term_scores.items():\n",
    "            term_idx = vocabulary.get(term)\n",
    "            if term_idx is not None:\n",
    "                tfidf_matrix[doc_idx, term_idx] = score\n",
    "    return tfidf_matrix\n",
    "\n",
    "# Prepare corpus\n",
    "corpus = df['processed_sentence'].tolist()\n",
    "\n",
    "# Compute TF-IDF scores\n",
    "tfidf, document_frequencies  = compute_tfidf(corpus)\n",
    "\n",
    "# Create vocabulary from all N-grams\n",
    "all_ngrams = set()\n",
    "for tf in tfidf.values():\n",
    "    all_ngrams.update(tf.keys())\n",
    "\n",
    "vocabulary = {term: idx for idx, term in enumerate(all_ngrams)}\n",
    "\n",
    "# Create TF-IDF matrix\n",
    "X = create_tfidf_matrix(tfidf, vocabulary)\n",
    "y = df['intent']\n",
    "\n",
    "# Find non-zero values in x\n",
    "# for i in range(len(X)):\n",
    "#     for j in range(len(X[i])):\n",
    "#         if X[i][j] != 0:\n",
    "#              print(X[i][j])\n",
    "#     break\n",
    "\n",
    "# print(y)\n",
    "\n",
    "#shape of y\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2296, 14960)\n",
      "(2296,)\n",
      "(574, 14960)\n",
      "(574,)\n"
     ]
    }
   ],
   "source": [
    "# Encode labels = 24\n",
    "labels = df['intent'].unique()\n",
    "# print(labels.shape)\n",
    "label_to_index = {label: idx for idx, label in enumerate(labels)}\n",
    "index_to_label = {idx: label for label, idx in label_to_index.items()}\n",
    "y = np.array([label_to_index[label] for label in y])\n",
    "# for i in range(len(y)):\n",
    "# \tprint(y[i])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict intent of custom input\n",
    "def predict_intent(custom_sentence, model):\n",
    "    # Preprocess the custom input\n",
    "    processed_sentence = preprocess_text(custom_sentence)\n",
    "    # Convert to N-gram TF-IDF features\n",
    "    ngrams = extract_ngrams(processed_sentence)\n",
    "    custom_tfidf = np.zeros(len(vocabulary))\n",
    "    term_counts = Counter(ngrams)\n",
    "    N = len(corpus) + 1\n",
    "    for term, count in term_counts.items():\n",
    "        if term in vocabulary:\n",
    "            term_idx = vocabulary[term]\n",
    "            tf = count / len(ngrams)\n",
    "            idf = math.log(N / (1 + document_frequencies.get(term, 0)))\n",
    "            custom_tfidf[term_idx] = tf * idf\n",
    "    # Predict the intent\n",
    "    predicted_intent_idx = model.predict(np.array([custom_tfidf]))[0]\n",
    "    predicted_intent = index_to_label[predicted_intent_idx]\n",
    "    return predicted_intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9407665505226481\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train a classifier (Logistic Regression)\n",
    "lg_model = LogisticRegression()\n",
    "lg_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = lg_model.predict(X_test)\n",
    "\n",
    "# Map index to label\n",
    "y_test_labels = [index_to_label[idx] for idx in y_test]\n",
    "y_pred_labels = [index_to_label[idx] for idx in y_pred]\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test_labels, y_pred_labels))\n",
    "# print(\"Classification Report:\\n\", classification_report(y_test_labels, y_pred_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted intent for 'i want a method named ADD ':\n",
      " Function Declaration\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Test the model with custom input\n",
    "custom_sentence = \"i want a method named ADD \"\n",
    "predicted_intent = predict_intent(custom_sentence, lg_model)\n",
    "print(f\"Predicted intent for '{custom_sentence}':\\n {predicted_intent}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.980836236933798\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "class MulticlassLogisticRegression:\n",
    "    def __init__(self, lr: float, epochs: int, probability_threshold: float = 0.5, random_state=None):\n",
    "        self.lr = lr  # The learning rate\n",
    "        self.epochs = epochs  # The number of training epochs\n",
    "        self.probability_threshold = probability_threshold  # Threshold for classification\n",
    "        self.random_state = random_state  # Seed for reproducibility\n",
    "        self.weights = []  # Store weights for each class\n",
    "\n",
    "    def _prepare_input(self, X):\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = np.array(X)\n",
    "        # Add a new input with value 1 to each example (bias term)\n",
    "        ones = np.ones((X.shape[0], 1), dtype=X.dtype)\n",
    "        return np.concatenate((ones, X), axis=1)\n",
    "\n",
    "    def _initialize(self, num_weights: int, stdev: float = 0.01):\n",
    "        # Initialize the weights using a normally distributed random variable with a small standard deviation\n",
    "        np.random.seed(self.random_state)\n",
    "        return np.random.randn(num_weights) * stdev\n",
    "\n",
    "    def _gradient(self, X, y, weights):\n",
    "        # Compute and return the gradient of the weights with respect to the loss given X and y\n",
    "        error = y - sigmoid(np.dot(X, weights))\n",
    "        weight_gradient = np.dot(-X.T, error)\n",
    "        return weight_gradient\n",
    "\n",
    "    def _update(self, X, y, weights):\n",
    "        # Apply a single iteration on the weights\n",
    "        gradient = self._gradient(X, y, weights)\n",
    "        weights -= self.lr * gradient\n",
    "        return weights\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = self._prepare_input(X)\n",
    "        encoder = LabelBinarizer()\n",
    "        y_oh = encoder.fit_transform(y)\n",
    "\n",
    "        for i in range(y_oh.shape[1]):\n",
    "            weights = self._initialize(X.shape[1])\n",
    "            for _ in range(self.epochs):\n",
    "                weights = self._update(X, y_oh[:, i], weights)\n",
    "            self.weights.append(weights)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = self._prepare_input(X)\n",
    "        probas = []\n",
    "        for weights in self.weights:\n",
    "            proba = sigmoid(np.dot(X, weights))\n",
    "            probas.append(proba)\n",
    "        return np.array(probas).T\n",
    "\n",
    "    def predict(self, X):\n",
    "        probas = self.predict_proba(X)\n",
    "        return np.argmax(probas, axis=1)\n",
    "    \n",
    "\n",
    "model = MulticlassLogisticRegression(lr=0.1, epochs=500, probability_threshold=0.5, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted intent for 'create a file named 'test.txt'':\n",
      " File System\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the model with custom input\n",
    "custom_sentence = \"create a file named 'test.txt'\"\n",
    "predicted_intent = predict_intent(custom_sentence, model)\n",
    "print(f\"Predicted intent for '{custom_sentence}':\\n {predicted_intent}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "import pickle\n",
    "with open('logisticRegressionModel.pkl', 'wb') as file:\n",
    "\tpickle.dump(model, file)\n",
    "\n",
    "#Load the model\n",
    "with open('logisticRegressionModel.pkl', 'rb') as file:\n",
    "\tmodel = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        48\n",
      "           1       1.00      1.00      1.00        46\n",
      "           2       1.00      1.00      1.00        23\n",
      "           3       1.00      1.00      1.00         8\n",
      "           4       1.00      0.83      0.91        12\n",
      "           5       0.95      1.00      0.97        36\n",
      "           6       0.95      1.00      0.98        21\n",
      "           7       0.92      1.00      0.96        12\n",
      "           8       1.00      0.95      0.98        22\n",
      "           9       1.00      0.95      0.97        20\n",
      "          10       0.95      1.00      0.97        77\n",
      "          11       1.00      1.00      1.00        33\n",
      "          12       0.94      1.00      0.97        15\n",
      "          13       1.00      1.00      1.00        11\n",
      "          14       1.00      1.00      1.00        17\n",
      "          15       1.00      0.50      0.67         6\n",
      "          16       1.00      0.75      0.86         4\n",
      "          17       0.96      0.92      0.94        25\n",
      "          18       0.98      1.00      0.99        53\n",
      "          19       1.00      0.91      0.95        11\n",
      "          20       1.00      1.00      1.00         7\n",
      "          21       1.00      1.00      1.00         6\n",
      "          22       1.00      1.00      1.00        44\n",
      "          23       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           0.98       574\n",
      "   macro avg       0.99      0.95      0.96       574\n",
      "weighted avg       0.98      0.98      0.98       574\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classRep = classification_report(y_test,y_pred)\n",
    "print(classRep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robinenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
